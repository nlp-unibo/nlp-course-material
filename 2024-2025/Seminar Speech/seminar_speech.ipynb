{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar: The Language of Sound: A Journey Through Speech Processing and Multimodality\n",
    "- Credits: Eleonora Mancini \n",
    "- Keywords: Speech Processing, Speech Foundation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generating Spectrogram from Waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processing with Librosa\n",
    "In this section, we will use **Librosa** to load an audio file, play it, and visualize the waveform, spectrogram, and mel spectrogram.\n",
    "\n",
    "Note that the same operations can be performed using **torchaudio**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "1. Load the audio file using Librosa.\n",
    "2. Play the audio.\n",
    "3. Plot the waveform of the audio.\n",
    "4. Compute and display the spectrogram.\n",
    "5. Compute and display the mel spectrogram.\n",
    "\n",
    "Before proceeding, make sure you have the required libraries installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa matplotlib numpy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an Audio File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = 'audio_files/hello_sound.wav'  # Replace with your file path\n",
    "y, sr = librosa.load(audio_path, sr=None)  # y is the audio signal, sr is the sample rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce the Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and Display the Spectrogram\n",
    "### Understanding the STFT in Librosa\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **STFT and Segments**:  \n",
    "   When computing an STFT, the signal is divided into several short segments, each of length `n_fft`. The FFT is then computed for each segment. To avoid losing information, these segments often overlap, meaning the distance between consecutive segments is typically **less than `n_fft`**. This overlap is controlled by the `hop_length`, which is the number of audio samples between the starts of consecutive FFTs.\n",
    "\n",
    "2. **Hop Length in Samples**:  \n",
    "   The `hop_length` is defined in samples. For example:\n",
    "   - If you have 1000 audio samples and `hop_length` = 100, you get **10 feature frames**.  \n",
    "   - If `n_fft` > `hop_length`, padding may be applied to ensure all samples are processed.\n",
    "\n",
    "3. **Frame Rate**:  \n",
    "   The rate at which feature frames are generated can be calculated as:  \n",
    "   $$\\text{frame\\_rate} = \\frac{\\text{sample\\_rate}}{\\text{hop\\_length}}$$  \n",
    "   For example, with:\n",
    "   - `sample_rate` = 22050 Hz  \n",
    "   - Default `hop_length` = 512  \n",
    "\n",
    "   The frame rate is:  \n",
    "   $$\\frac{22050}{512} \\approx 43 \\, \\text{frames per second}$$\n",
    "\n",
    "4. **Output Dimensions**:  \n",
    "   For an audio clip of 10 seconds at 22050 Hz, the resulting spectrogram will have dimensions of approximately:\n",
    "   - **Frequency bins**: Defined by the Mel filterbank (e.g., 128 Mel bins).  \n",
    "   - **Feature frames**: Determined by the audio duration and the `hop_length`.  \n",
    "\n",
    "   Example:\n",
    "   - Audio duration = 10s  \n",
    "   - Sample rate = 22050 Hz  \n",
    "   - `hop_length` = 512  \n",
    "\n",
    "   Number of feature frames:  \n",
    "   $$\\text{frames} = \\frac{\\text{audio\\_samples}}{\\text{hop\\_length}} = \\frac{22050 \\times 10}{512} \\approx 430$$\n",
    "\n",
    "   Resulting spectrogram dimensions:  \n",
    "   $$(128, 430)$$  \n",
    "   where 128 is the number of Mel bins, and 430 is the number of feature frames.  \n",
    "   Note: **Padding** can slightly alter these dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "### With and Without Mel Bins\n",
    "\n",
    "#### Without Mel Bins:\n",
    "- **Frequency Representation**:  \n",
    "  The spectrogram represents the **raw linear frequency scale** directly from the STFT. The number of frequency bins is:  \n",
    "  $$\\text{n\\_fft} / 2 + 1$$  \n",
    "  For example, if \\( \\text{n\\_fft} = 1024 \\), there are \\( 1024 / 2 + 1 = 513 \\) frequency bins.\n",
    "\n",
    "- **Resolution**:  \n",
    "  All frequencies are equally spaced, which does not match the logarithmic nature of human hearing. This is suitable for applications requiring precise frequency information, such as **pitch detection** or **music transcription**, but less interpretable for tasks involving human perception.\n",
    "\n",
    "- **Output Dimensions**:  \n",
    "  Spectrogram dimensions will be \\((\\text{n\\_fft}/2+1, \\text{num\\_frames})\\), where `num_frames` depends on the audio duration and `hop_length`.\n",
    "\n",
    "---\n",
    "\n",
    "#### With Mel Bins:\n",
    "- **Frequency Representation**:  \n",
    "  Frequencies are mapped to the **Mel scale**, which is perceptually motivated. Lower frequencies have finer resolution, and higher frequencies are compressed.\n",
    "\n",
    "- **Reduced Dimensionality**:  \n",
    "  A Mel filterbank reduces the number of frequency bins to a fixed number (e.g., 128 or 40). This reduces computational complexity and focuses on perceptually relevant features.\n",
    "\n",
    "- **Output Dimensions**:  \n",
    "  Spectrogram dimensions become \\((\\text{num\\_mel\\_bins}, \\text{num\\_frames})\\), where `num_mel_bins` is user-defined.\n",
    "\n",
    "- **Applications**:  \n",
    "  Mel-scaled spectrograms are better suited for tasks like **speech recognition**, **emotion detection**, and other tasks involving human perception.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)  # Compute the STFT and convert to dB\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(D, x_axis='time', y_axis='log', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and display the Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)  # Mel spectrogram\n",
    "\n",
    "# Convert to decibels\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "librosa.display.specshow(S_db, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do We Need to Perform `AmplitudeToDB`?\n",
    "In audio signal processing, converting the amplitude of an audio signal into a decibel (dB) scale is a common practice for a few key reasons:\n",
    "\n",
    "1. **Human Hearing Perception**: The human auditory system perceives sound in a logarithmic manner. This means that we do not perceive differences in sound intensity (loudness) in a linear way. A sound that is 10 times more intense than another is not perceived as 10 times louder. Instead, it is perceived in a more compressed scale. The decibel scale closely matches this perception, making it more intuitive when working with audio data.\n",
    "\n",
    "2. **Dynamic Range Compression**: Audio signals, especially raw waveforms, often have a wide dynamic range. That is, they contain both very quiet and very loud sections. A signal‚Äôs dynamic range can span several orders of magnitude. By converting amplitudes into dB, we compress this range, making it easier to visualize and analyze without losing important details. This is particularly useful for operations like spectrograms, where low-energy components would otherwise be hard to distinguish from noise.\n",
    "\n",
    "3. **Logarithmic Representation**: The decibel scale is logarithmic, which helps compress very large values and make the differences more noticeable in a manageable range. For instance, very large or very small values (in terms of amplitude) are normalized into a range that is easier to visualize or process. This can help reveal patterns in the data that would be difficult to detect in a purely linear scale.\n",
    "\n",
    "4. **Standard Practice in Audio Analysis**: Many audio and speech processing algorithms (such as in speech recognition, audio classification, or music information retrieval) are designed to work with logarithmic representations of audio, like spectrograms in decibels. Converting to dB ensures consistency and comparability across various tools, datasets, and research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process: `AmplitudeToDB` in Detail\n",
    "When you perform an operation like the Short-Time Fourier Transform (STFT) or Mel-frequency Cepstral Coefficients (MFCCs) on an audio signal, the result is typically a **magnitude spectrogram**. This magnitude is in linear units, representing raw amplitudes.\n",
    "\n",
    "To convert the raw amplitude spectrogram to a logarithmic scale, we use AmplitudeToDB, which transforms the amplitude values into decibel (dB) values using the formula:\n",
    "\n",
    "$ùëëùêµ=10√ólog_{10}(P)$\n",
    "\n",
    "Where:\n",
    "- $P$ is the power or amplitude of the signal.\n",
    "This transformation helps improve the perceptual relevance of the data and aids in better visualization of the frequency content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives to `AmplitudeToDB`\n",
    "\n",
    "While `AmplitudeToDB` is a widely used and standard method for converting amplitude to decibels (dB), there are several alternative approaches or transformations that can be used depending on the context:\n",
    "\n",
    "\n",
    "#### **1. Logarithmic Scaling (Without `AmplitudeToDB`)**\n",
    "You can apply a direct logarithmic transformation to the magnitude spectrogram or waveform. This method typically operates on the **power** of the signal, rather than just the amplitude. For example:\n",
    "\n",
    "\\[\n",
    "\\text{log\\_spec} = \\log(\\text{spec} + \\epsilon)\n",
    "\\]\n",
    "\n",
    "Here, \\( \\epsilon \\) is a small constant added to avoid taking the logarithm of zero.\n",
    "\n",
    "This approach can approximate decibel scaling but lacks the specific scaling factor (e.g., \\( 10 \\times \\) or \\( 20 \\times \\)) applied by `AmplitudeToDB`. It is less common than using `AmplitudeToDB`, which is specifically designed for audio data.\n",
    "\n",
    "#### **4. Mel Spectrogram**\n",
    "Instead of converting to dB after computing the spectrogram, you can directly compute a **Mel spectrogram**. This method applies a non-linear transformation to map frequencies to the Mel scale, which better aligns with human auditory perception.\n",
    "\n",
    "The Mel spectrogram can also be combined with logarithmic scaling to produce **Mel-frequency cepstral coefficients (MFCCs)**, which are widely used in speech and audio processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Accessing and Fine-Tuning Speech Foundation Models with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inference Example: using Pre-trained Wav2Vec2 Model\n",
    "In this section, we will demonstrate how to perform inference using a pre-trained speech model, specifically Wav2Vec2. We will load an audio file, process it, and then use the model to transcribe the speech to text. This will showcase how to quickly leverage powerful models from Hugging Face for automatic speech recognition (ASR) without needing to train the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torchaudio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained Wav2Vec2 model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and resample audio with PyTorch\n",
    "\n",
    "To handle audio files that may have a sampling rate different from the one used in the pre-trained Wav2Vec2 model (which was trained on 16 kHz audio), we need to add a resampling step to convert the audio to the correct sampling rate before passing it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load audio\n",
    "def load_and_resample_audio(file_path, target_sample_rate=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)  # Load audio file\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)  # Resample to target sample rate\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to transcribe speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(waveform):\n",
    "    # Step 1: Preprocess the waveform (audio) using the processor\n",
    "    # The processor tokenizes and prepares the input waveform to be fed into the model.\n",
    "    # It expects the audio to have a sampling rate of 16 kHz and converts the waveform into a tensor.\n",
    "    # 'padding=True' ensures that the inputs are padded to the correct length.\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Step 2: Print the shape of the input tensor\n",
    "    # This prints the shape of the processed audio tensor to see its dimensions before feeding it into the model.\n",
    "    print(inputs.input_values.shape)\n",
    "    \n",
    "    # Step 3: Forward pass through the model\n",
    "    # With torch.no_grad() to prevent gradients from being calculated (since we are not training the model).\n",
    "    # This step passes the processed input values through the model to get the logits (raw output scores).\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values=inputs.input_values).logits\n",
    "        # logits are the unnormalized predictions made by the model.\n",
    "        # They will be used to generate the final predicted transcription.\n",
    "        print(logits)\n",
    "    \n",
    "    # Step 4: Decode predictions\n",
    "    # `logits` contain the output predictions of the model. We use `torch.argmax` to get the index of the maximum logit.\n",
    "    # This gives us the predicted token IDs (word/phoneme IDs) for the input sequence.\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # The 'dim=-1' indicates we are taking the max value across the last dimension.\n",
    "    \n",
    "    # Step 5: Convert the predicted token IDs to text\n",
    "    # The processor.decode function converts the token IDs back into a human-readable transcription (string).\n",
    "    # Here, we only decode the first (and typically the only) batch in the tensor.\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "    \n",
    "    # Step 6: Return the transcription\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"audio_files/hello_sound.wav\"  # Update with your audio file path\n",
    "waveform, sr = load_and_resample_audio(file_path)\n",
    "transcription = transcribe_audio(waveform)\n",
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use SFMs to Generate Representations: Extracting Embeddings \n",
    "In this section, we will demonstrate how to extract embeddings from a pre-trained speech model that can be used for downstream tasks such as speech classification. We will use the Wav2Vec2 model to extract meaningful audio representations (embeddings). These embeddings can be then directly input into a separate classifier for tasks like speaker identification or emotion recognition in speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Wav2Vec2 model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load audio\n",
    "def load_and_resample_audio(file_path, target_sample_rate=16000):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)  # Load audio file\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)  # Resample to target sample rate\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract embeddings from the model\n",
    "def extract_embeddings(inputs):\n",
    "\n",
    "    # Get embeddings (hidden states) from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_values=inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Extract hidden states (embeddings) from the last layer\n",
    "    hidden_states = outputs.hidden_states[-1]\n",
    "    embeddings = torch.mean(hidden_states, dim=1)  # Optionally, take the mean of the hidden states for each time step\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for embedding extraction\n",
    "file_path = \"audio_files/hello_sound.wav\"  # Update with your audio file path\n",
    "waveform = load_and_resample_audio(file_path)\n",
    "embeddings = extract_embeddings(waveform)\n",
    "\n",
    "# Now embeddings can be used as input to a downstream classification model\n",
    "print(\"Extracted embeddings:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
