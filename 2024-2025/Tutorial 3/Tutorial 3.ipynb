{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248026a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial 3\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Transformers, Huggingface, Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f656c",
   "metadata": {
    "id": "m3wzWLL-LiKd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c15ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART 0 ($\\sim$5 mins)\n",
    "*   Downloading a **dataset**.\n",
    "*   Encoding a a **dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe496b",
   "metadata": {
    "id": "gl48Am5trp3Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART I ($\\sim$30 mins)\n",
    "\n",
    "*   Text encoding with transformers.\n",
    "*   Model definition.\n",
    "*   Model training and evaluation with huggingface APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3fbeb",
   "metadata": {
    "id": "D4anSmM4rp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART II ($\\sim$30 mins)\n",
    "\n",
    "*   Prompting 101\n",
    "*   Sentiment analysis with prompting\n",
    "*   Advanced prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1151d4",
   "metadata": {
    "id": "c4-E45fvrp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First of all, we need to import some useful packages that we will use during this hands-on session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66be93a",
   "metadata": {
    "id": "rUXZLYya69wc",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# system packages\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import sys\n",
    "\n",
    "# data and numerical management packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# useful during debugging (progress bars)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9d293",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install evaluate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c7b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954d2c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305b6d5c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 2560,\n",
    "        'height': 1440,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932eb9e",
   "metadata": {
    "id": "qImKj2Mu7LCX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "We will use the IMDB dataset first introduced in tutorial 1.\n",
    "\n",
    "* [**Stats**] A dataset of 50k sentences used for sentiment analysis: 25k with positive sentiment, 25k with negative one.\n",
    "* [**Sentiment**] We consider sentiment labels for classification.\n",
    "\n",
    "We start by **downloading** the dataset and **extract** it to a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdf63f",
   "metadata": {
    "id": "NSvqBcKJ7iTY",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with tarfile.open(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(extract_path)\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11619d9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset_name = \"aclImdb\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_tar_path = dataset_folder.joinpath(\"Movies.tar.gz\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_tar_path.exists():\n",
    "    download_dataset(dataset_tar_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_tar_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b5072",
   "metadata": {
    "id": "pv3NW1SNrp3a",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data Format\n",
    "\n",
    "Just like in the first assignment, we need a **high level view** of the dataset that is helpful to our needs. \n",
    "\n",
    "We encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db381745",
   "metadata": {
    "id": "P05YfYCe7qCj",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        folder = dataset_folder.joinpath(dataset_name, split, sentiment)\n",
    "        for file_path in folder.glob('*.txt'):            \n",
    "            with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
    "                text = text_file.read()\n",
    "                score = file_path.stem.split(\"_\")[1]\n",
    "                score = int(score)\n",
    "                file_id = file_path.stem.split(\"_\")[0]\n",
    "\n",
    "                num_sentiment = 1 if sentiment == 'pos' else 0\n",
    "\n",
    "                dataframe_row = {\n",
    "                    \"file_id\": file_id,\n",
    "                    \"score\": score,\n",
    "                    \"sentiment\": num_sentiment,\n",
    "                    \"split\": split,\n",
    "                    \"text\": text\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584277b8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\", dataset_name)\n",
    "if not folder.exists():\n",
    "    folder.mkdir(parents=True)\n",
    "\n",
    "# transform the list of rows in a proper dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "df = df[[\"file_id\", \n",
    "         \"score\",\n",
    "         \"sentiment\",\n",
    "         \"split\",\n",
    "         \"text\"]\n",
    "       ]\n",
    "df_path = folder.with_name(dataset_name + \".pkl\")\n",
    "df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecf8a6",
   "metadata": {
    "id": "Bjf3k-qVrp3b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART I\n",
    "\n",
    "*   Text encoding with Transformers.\n",
    "*   Model definition.\n",
    "*   Model training and evaluation with huggingface APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c6a6d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Text encoding with Transformers.\n",
    "\n",
    "In tutorial 1, we have seen how to define standard machine learning models to address sentiment classification.\n",
    "\n",
    "However, we know that Transformer-based models are one of the strongest baselines when assessing a task or benchmarking on a novel corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bac417",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before defining our transformer-based classifier, we need to encode text inputs into numerical format.\n",
    "\n",
    "As in Tutorial 1, we are going to **tokenize** input texts to perform token indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375eb13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Encoding the dataset\n",
    "\n",
    "First, we are going to use ``datasets`` library to encode our dataset into a handy wrapper for computational speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808d5bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Slicing for showcasing purposes only!\n",
    "train_df = df.loc[df['split'] == \"train\"].sample(frac=1.0)[:5000]\n",
    "test_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:1000]\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87decf3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the newly defined `Dataset` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881491ae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c1326",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Tokenization\n",
    "\n",
    "Transformers typically use [SentencePiece tokenizer](https://github.com/google/sentencepiece) to perform sub-word level tokenization.\n",
    "\n",
    "In particular, the `transformers` library offers the `AutoTokenizer` class to quickly retrieve our chosen transformer's ad-hoc tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b30591",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_card = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a7224",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `model_card` variable defines the *path* where to look for our pre-trained model.\n",
    "\n",
    "You can check [huggingface's hub](https://huggingface.co/models) model hub to pick the model card according to your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb5e33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We proceed on tokenizing movie reviews text with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc272c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    return tokenizer(texts['text'], truncation=True)\n",
    "\n",
    "train_data = train_data.map(preprocess_text, batched=True)\n",
    "test_data = test_data.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb9a10",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the preprocess `Dataset` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a668aae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce543485",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data['input_ids'][50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6246b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(train_data['attention_mask'][50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a13d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can perform some quick *sanity check* to evaluate the tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f216e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "original_text = train_data['text'][50]\n",
    "decoded_text = tokenizer.decode(train_data['input_ids'][50])\n",
    "\n",
    "print(original_text)\n",
    "print()\n",
    "print()\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b0876",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 Vocabulary\n",
    "\n",
    "We **do not** necessarily need to build a vocabulary since transformers already come with their own! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0050d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**However**, it is still possible to add new tokens to the vocabulary to adapt the model to the given use case.\n",
    "\n",
    "```\n",
    "tokenizer.add_tokens(new_tokens=new_tokens)\n",
    "```\n",
    "\n",
    "The transformer vocabulary will update its **unusued** vocabulary indexes with newly provided tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ad541",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.4 Special tokens\n",
    "\n",
    "**Pay attention** to used special tokens and their corresponding token ids.\n",
    "\n",
    "Each transformer models has its own special tokens ([CLS], [SEP], [PAD], [EOS], etc...).\n",
    "\n",
    "Thus, the same special token may be mapped to different token ids in distinct transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db7d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.5 Text cleaning\n",
    "\n",
    "We didn't perform any kind of text cleaning before performing text encoding.\n",
    "\n",
    "This is usually because transformer tokenizers **have their own text cleaning process** to perform tokenization.\n",
    "\n",
    "Thus, models **may be sensitive** to custom operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a569f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"couldn't\"\n",
    "encoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\n",
    "print(encoded_example.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793cb07a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"At one point,some kids are wandering through the deeper levels, exploring.\"\n",
    "encoded_example = tokenizer.encode_plus(example_text, add_special_tokens=False)\n",
    "print(encoded_example.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b421985",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "`bert-base-uncased` is trained with text in lower format.\n",
    "\n",
    "**Check model cards** on huggingface to know more about the models you use and inspect their text encoding pipeline to understand how they behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45979e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework ðŸ“–\n",
    "\n",
    "Experiment with different model cards.\n",
    "\n",
    "Experiment with text cleaning and evaluate its impact on classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da8bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Model definition\n",
    "\n",
    "We are now ready to define our transformer-based classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a686b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Data Formatting\n",
    "\n",
    "We first need to format input data to be fed as mini-batches in a training/evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54f36f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc44786",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ``DataCollatorWithPadding`` receives a batch of\n",
    "\n",
    "```\n",
    "(input_ids, attention_mask, token_type_ids, label)\n",
    "```\n",
    "\n",
    "tuples and **dynamically pads** ``input_ids``, ``attention_mask`` and ``token_type_ids`` to maximum sequence in the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8705453",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, this operation saves a lot of memory compared to padding to global maximum sequence, while it introduces a reasonable computational overhead.\n",
    "\n",
    "### Note\n",
    "\n",
    "The above example is just one way out of many to perform dynamic batch padding: it really depends on which data structures you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46c5b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Model definition\n",
    "\n",
    "Defining a transformer-based model with huggingface is pretty straightforward!\n",
    "\n",
    "Since we are dealing with text classification, we can use off-the-shelf `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7888a387",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_card,\n",
    "                                                           num_labels=2,\n",
    "                                                           id2label={0: 'NEG', 1: 'POS'},\n",
    "                                                           label2id={'NEG': 0, 'POS': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d8ded",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first check the loaded model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378fff3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e6fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "That's the simplicity of huggingface's APIs.\n",
    "\n",
    "The model is ready to use for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598b147",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3 Custom architectures\n",
    "\n",
    "There are plenty of pre-defined model architectures $\\rightarrow$ [auto classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "\n",
    "In more complex scenarios, we may want to define a custom architecture where the pre-trained model is part of it.\n",
    "\n",
    "In these cases, the way you do it strongly depends on the underlying neural library.\n",
    "\n",
    "However, there exist several high-level APIs depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd8e31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Model training and evaluation\n",
    "\n",
    "We are now ready to define the training and evaluation procedures to test our model on the IMDB dataset.\n",
    "\n",
    "In particular, we are going to use ``Trainer`` APIs to efficiently perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fdb52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 Metrics\n",
    "\n",
    "First, we define classification metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3550e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    return {'f1': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e2ad2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hugginface's metrics\n",
    "\n",
    "Huggingface's offers the **evaluate** package that contains several evaluation metrics (e.g., accuracy, f1, squad-f1, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d90dd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    return {**f1, **acc}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb4224",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 Training Arguments\n",
    "\n",
    "The ``Trainer`` object can be extensively customized.\n",
    "\n",
    "Feel free to check the [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) on training arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b2df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first rename the `sentiment` column to `label` as the default input to `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002dcef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.rename_column('sentiment', 'label')\n",
    "test_data = test_data.rename_column('sentiment', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9b30a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_dir\",                 # where to save model\n",
    "    learning_rate=2e-5,                   \n",
    "    per_device_train_batch_size=8,         # accelerate defines distributed training\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",           # when to report evaluation metrics/losses\n",
    "    save_strategy=\"epoch\",                 # when to save checkpoint\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'                       # disabling wandb (default)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4071868",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f56baf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training schema with collator\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/collator.png\" alt=\"collator\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37827762",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bbd39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Evaluation\n",
    "\n",
    "We now evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacd3fa",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_prediction_info = trainer.predict(test_data)\n",
    "test_predictions, test_labels = test_prediction_info.predictions, test_prediction_info.label_ids\n",
    "\n",
    "print(test_predictions.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07749c06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_metrics = compute_metrics([test_predictions, test_labels])\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54212aed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some cleaning before PART II\n",
    "\n",
    "Let's clean the memory and GPU before switching to instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af716d88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model = None\n",
    "del model\n",
    "trainer = None\n",
    "del trainer\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd0f67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART II\n",
    "\n",
    "*   Prompting 101\n",
    "*   Sentiment analysis with prompting\n",
    "*   Advanced prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd914bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Prompting 101\n",
    "\n",
    "Prompting is a technique used to adapt a model to a variety of tasks without requiring fine-tuning.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215c9d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model receives the above input prompt and performs text classification via completion.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: {text}\n",
    "Sentiment: {label}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac2bcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In natural language, prompting is a very delicate process since natural language is **expressive**, **flexible**, and, **ambiguous**.\n",
    "\n",
    "A certain concept can be expressed in several ways:\n",
    "\n",
    "* These ways are semantically **equivalent**\n",
    "* May lead to **significant** model performance **drifts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4af6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Sensitivity Factors\n",
    "\n",
    "There are two main factors to consider when performing prompt-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824d676",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Prompt Engineering](https://www.promptingguide.ai/)\n",
    "\n",
    "Eventually we have to iteratively find the best performing prompt.\n",
    "\n",
    "This can either done\n",
    "\n",
    "* Manually\n",
    "* Automatically (via an ad-hoc model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaed127",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Generation hyper-parameters](https://huggingface.co/docs/transformers/main/generation_strategies#text-generation-strategies)\n",
    "\n",
    "Finding the optimal text generation strategy is a **critical point** for achieving satisfying performance.\n",
    "\n",
    "These strategies affects how the model iteratively selects tokens during generation to avoid phenomena like repetitions, rare words, coherence with input text, and style.\n",
    "\n",
    "* [Deterministic] Greedy $\\rightarrow$ the most preferred (i.e., highest likelihood) token wins\n",
    "* [Deterministic] Beam search\n",
    "* [Stochastic] Top-k sampling\n",
    "* [Stochastic] Nucleus sampling (or top-p sampling)\n",
    "* [Contrastive search](https://huggingface.co/blog/introducing-csearch)  $\\leftarrow$ **recommended**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3603382",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Model types\n",
    "\n",
    "There are a lot of different large language models and it is quite easy to be confused.\n",
    "\n",
    "Essentially, we have:\n",
    "\n",
    "* **Base models** (either encoders or encode-decoders): very good at text completion.\n",
    "* **Instruct-based models**: base models specifically fine-tuned to address instructions.\n",
    "* **Chat-based models**: models specifically fine-tuned to chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43bd15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "In Huggingface, the distinct is easily formatted as:\n",
    "\n",
    "* `llama2-7b`            $\\rightarrow$ base model\n",
    "* `llama2-7b-instruct`   $\\rightarrow$ instruct-based model\n",
    "* `llama-7b-chat`        $\\rightarrow$ chat-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65574c39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 Preliminaries\n",
    "\n",
    "We are going to download LLMs from [Huggingface](https://huggingface.co/).\n",
    "\n",
    "Many of these open-source LLMs require you to accept their \"Community License Agreement\" to download them.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- If not already, create an account of Huggingface (~2 mins)\n",
    "- Check a LLM model card page (e.g., [Mistral v3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)) and accept its \"Community License Agreement\".\n",
    "- Go to your account -> Settings -> Access Tokens -> Create new token -> \"Repositories permissions\" -> add the LLM model card you want to use.\n",
    "- Save the token (we'll need it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4672f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once we have created an account and an access token, we need to login to Huggingface via code.\n",
    "\n",
    "- Type your token and press Enter\n",
    "- You can say No to Github linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc426bbc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfc4e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After login, you can download all models associated with your access token in addition to those that are not protected by an access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791323f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Sentiment analysis with prompting\n",
    "\n",
    "Let's consider our task once again to evaluate prompt-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115caa1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Model pipeline\n",
    "\n",
    "First, we have to define the model pipeline to digest input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70442c79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_card = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f6ffef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_card,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30325e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 100\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.temperature = None\n",
    "generation_config.num_return_sequences = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ca902",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework ðŸ“–\n",
    "\n",
    "Experiment with different model cards (either base or chat-base models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3af120",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Prompt Template\n",
    "\n",
    "We first define the prompt template to format data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718ea96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sentiment analysis.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Classify the text into negative or positive.\n",
    "        Respond only POSITIVE or NEGATIVE.\n",
    "\n",
    "        TEXT: \n",
    "        {text}\n",
    "\n",
    "        SENTIMENT:\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b0614",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the formatted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28371a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865b844",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Inference\n",
    "\n",
    "We are now ready to feed prompts to our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66afd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb7567",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\"\n",
    "formatted_example = prompt.format(text=example_text)\n",
    "parsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786c2f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we try with the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61982dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:100]\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b083ec89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepend_prompt(example):\n",
    "    example['text'] = prompt.format(text=example['text'])\n",
    "    return example\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = tokenizer.batch_encode_plus([it['text'] for it in batch], return_tensors='pt', padding=True, truncation=True)\n",
    "    sentiment = th.tensor([it['sentiment'] for it in batch])\n",
    "    return texts, sentiment\n",
    "\n",
    "test_data = test_data.map(prepend_prompt)\n",
    "test_data = test_data.select_columns(['text', 'sentiment'])\n",
    "data_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02e971",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before running the inference loop, we define a function to parse generated outputs into classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18baa5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_response(response):\n",
    "    match = [m for m in re.finditer('SENTIMENT:', response)][-1]\n",
    "    parsed = response[match.end():].strip()\n",
    "    return parsed\n",
    "\n",
    "def convert_response(response):\n",
    "    return [0, 1] if 'positive' in response.casefold() else [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d73a78",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "raw_responses = []\n",
    "predictions = []\n",
    "with th.inference_mode():\n",
    "    for batch_x, batch_y in tqdm(data_loader, desc=\"Generating responses\"):\n",
    "        response = model.generate(\n",
    "            input_ids=batch_x['input_ids'].to(model.device),\n",
    "            attention_mask=batch_x['attention_mask'].to(model.device),\n",
    "            generation_config=generation_config,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        raw_response = tokenizer.batch_decode(response, skip_special_tokens=True)\n",
    "        raw_response = [extract_response(item) for item in raw_response]\n",
    "        raw_responses.extend(raw_response)\n",
    "        batch_predictions = [convert_response(item) for item in raw_response]\n",
    "        predictions.extend(batch_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fac0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now compute classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3a234",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(test_data['sentiment'])\n",
    "metrics = compute_metrics([predictions, ground_truth])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f128c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. [Advanced Prompting](https://huggingface.co/docs/transformers/main/tasks/prompting#chain-of-thought)\n",
    "\n",
    "There is no rule of thumb to perform well on prompting.\n",
    "\n",
    "Some may argue it is *art*, some others might say it is just *engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8b278",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, here are some **general recommendations**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e5b1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Check **how** the pre-trained model you are using was trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71db06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Start **simple** and then refine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46a65e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instructions at the **start/end** of the prompt $\\rightarrow$ based on how most attention layers work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49fcee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Separate** input text from instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e007b29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Provide **clear description** of the task: no ambiguity, text format, style, language, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab1762",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Evaluate** the prompt on several models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e669802",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use advanced techniques: **few-shot prompting**, **Chain-of-thought (CoT)**, Least-to-Most (LtM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27c309",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 From Zero- to Few-shot Prompting\n",
    "\n",
    "In many situations, a prompt containing instructions is not sufficient for a model to behave properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52c86c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can improve the prompt by providing **a few** ground-truth examples showing how the model should behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedb54a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Classify the text into negative or positive. \n",
    "Text: {example1}\n",
    "Sentiment: {label1}\n",
    "Text: {example2}\n",
    "Sentiment: {label2}\n",
    "Text: {example3}\n",
    "Sentiment: {label3}\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494c4ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples may be insufficient\n",
    "\n",
    "Depending on the task at hand, providing examples may be not sufficient for the model to *understand* the instructions.\n",
    "\n",
    "Also, the model might ignore provided examples or it might still perform correctly despite using **intentionally wrong** examples!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42471d5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lengthy prompts\n",
    "\n",
    "Adding examples increases the level of detail of prompt, while it may considerably increases its length.\n",
    "\n",
    "Pay attention to what ``model_card`` you choose since your model may **truncate** input prompts!\n",
    "\n",
    "Additionally, a lengthy prompt **increases computation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760aaae9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples quality\n",
    "\n",
    "Choosing the right set of examples has an impact on model performance.\n",
    "\n",
    "Intuitively, we select examples to maximize (textual) diversity and cover the whole label distribution.\n",
    "\n",
    "In practice, this may be harder than expected: models are sensitive to prompt formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec77bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try sentiment analysis again with Few-shot prompting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130e78a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sentiment analysis.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Classify the text into negative or positive.\n",
    "        Respond only POSITIVE or NEGATIVE.\n",
    "        \n",
    "        Here are some examples you can look at.\n",
    "        EXAMPLES:\n",
    "        {examples}\n",
    "\n",
    "        Here is the text to classify.\n",
    "\n",
    "        TEXT: \n",
    "        {text}\n",
    "\n",
    "        SENTIMENT:\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3ef426",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "demonstrations = [\n",
    "    (\"Everything is so well done: acting, directing, visuals, settings, photography, casting. If you can enjoy a story of real people and real love - this is a winner.\", \"POSITIVE\"),\n",
    "    (\"This is one of the dumbest films, I've ever seen. It rips off nearly ever type of thriller and manages to make a mess of them all.\", \"NEGATIVE\")\n",
    "]\n",
    "demonstrations = '\\n'.join([f'TEXT: {text}\\nSENTIMENT: {sentiment}' for (text, sentiment) in demonstrations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bad65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\"\n",
    "formatted_example = prompt.format(text=example_text, examples=demonstrations)\n",
    "parsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c5d77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def prepend_prompt(example):\n",
    "    example['text'] = prompt.format(text=example['text'], examples=demonstrations)\n",
    "    return example\n",
    "\n",
    "test_data = Dataset.from_pandas(test_df)\n",
    "test_data = test_data.map(prepend_prompt)\n",
    "test_data = test_data.select_columns(['text', 'sentiment'])\n",
    "data_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca5008",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "raw_responses = []\n",
    "predictions = []\n",
    "with th.inference_mode():\n",
    "    for batch_x, batch_y in tqdm(data_loader, desc=\"Generating responses\"):\n",
    "        response = model.generate(\n",
    "            input_ids=batch_x['input_ids'].to(model.device),\n",
    "            attention_mask=batch_x['attention_mask'].to(model.device),\n",
    "            generation_config=generation_config,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        raw_response = tokenizer.batch_decode(response, skip_special_tokens=True)\n",
    "        raw_response = [extract_response(item) for item in raw_response]\n",
    "        raw_responses.extend(raw_response)\n",
    "        batch_predictions = [convert_response(item) for item in raw_response]\n",
    "        predictions.extend(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c1c3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(test_data['sentiment'])\n",
    "metrics = compute_metrics([predictions, ground_truth])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f255b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework ðŸ“–\n",
    "\n",
    "Experiment with different few-shot examples and evaluate corresponding model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3412321",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 Chain-of-thought (CoT) Prompting\n",
    "\n",
    "Providing examples to improve task performance may fail in complex scenarios like reasoning tasks.\n",
    "\n",
    "CoT prompting forces the model to generate intermediate reasoning steps before providing the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05358f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CoT can either be achieved via\n",
    "\n",
    "* Few-shot examples on how to perform *reasoning*\n",
    "* Defining the prompt to force *reasoning* (e.g., *let's think step by step*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298b790",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try our sentiment analysis task with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56678577",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sentiment analysis.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Classify the text into negative or positive.\n",
    "        Respond with POSITIVE or NEGATIVE.\n",
    "        Think step by step.\n",
    "        \n",
    "        Here are some examples you can look at.\n",
    "        EXAMPLES:\n",
    "        {examples}\n",
    "\n",
    "        Here is the text to classify.\n",
    "\n",
    "        TEXT: \n",
    "        {text}\n",
    "\n",
    "        SENTIMENT:\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46702439",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\"\n",
    "formatted_example = prompt.format(text=example_text, examples=demonstrations)\n",
    "parsed_example = tokenizer(formatted_example, return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae1858",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_data = Dataset.from_pandas(test_df)\n",
    "test_data = test_data.map(prepend_prompt)\n",
    "test_data = test_data.select_columns(['text', 'sentiment'])\n",
    "data_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b5690",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "raw_responses = []\n",
    "predictions = []\n",
    "with th.inference_mode():\n",
    "    for batch_x, batch_y in tqdm(data_loader, desc=\"Generating responses\"):\n",
    "        response = model.generate(\n",
    "            input_ids=batch_x['input_ids'].to(model.device),\n",
    "            attention_mask=batch_x['attention_mask'].to(model.device),\n",
    "            generation_config=generation_config,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        raw_response = tokenizer.batch_decode(response, skip_special_tokens=True)\n",
    "        raw_response = [extract_response(item) for item in raw_response]\n",
    "        raw_responses.extend(raw_response)\n",
    "        batch_predictions = [convert_response(item) for item in raw_response]\n",
    "        predictions.extend(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c895837",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(test_data['sentiment'])\n",
    "metrics = compute_metrics([predictions, ground_truth])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34584cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework ðŸ“–\n",
    "\n",
    "Experiment with different CoT prompts to enforce intermediate reasoning steps.\n",
    "\n",
    "For more details check this [page](https://www.promptingguide.ai/techniques/cot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5990dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Prompting vs Fine-tuning\n",
    "\n",
    "At last, we may wondering on which technique to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8c810",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In short, prompting comes at hand when transferring a pre-trained model on a domain that has some affinities with those seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f292f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other cases like:\n",
    "\n",
    "* Different domain\n",
    "* Sensitive data\n",
    "* Low-resource language\n",
    "* Domain-specific model constraints\n",
    "\n",
    "Fine-tuning is the preferred choice (to maximize improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e09c5",
   "metadata": {
    "id": "UbzMCMfprp3m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
