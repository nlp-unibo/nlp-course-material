{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rduSZj7b5eiP"
   },
   "source": [
    "# Tutorial 2\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Keywords**: Sparse/Dense Word embeddings, Neural Networks, Training practices, Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3wzWLL-LiKd"
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Andrea Galassi -> a.galassi@unibo.it\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl48Am5trp3Y"
   },
   "source": [
    "## PART I ($\\sim$25 mins)\n",
    "*   Building a **vocabulary**.\n",
    "*   Building a **word-word co-occurrence matrix**.\n",
    "*   Defining a **similarity metric**: cosine similarity.\n",
    "*   Embedding **visualization** and **analysis** of their semantic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4anSmM4rp3Z"
   },
   "source": [
    "## PART II ($\\sim$25 mins)\n",
    "\n",
    "*   Loading pre-trained **dense** word embeddings: Word2Vec, GloVe, FastText.\n",
    "*   Checking **out-of-vocabulary** (OOV) terms.\n",
    "*   **Handling** OOV terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yTGSKMprp3Z"
   },
   "source": [
    "## PART III ($\\sim$25 mins)\n",
    "*   Problem setting: **sentiment analysis task**.\n",
    "*   Setting up a **data pipeline**: from dataset loading to data conversion.\n",
    "*   **Model definition** (with Keras framework).\n",
    "*   Model training, inference and **evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDPNdjRlrp3Z"
   },
   "source": [
    "## PART IV ($\\sim$25 mins)\n",
    "\n",
    "*   **Regularization** techniques: L2, Dropout, Early Stopping.\n",
    "*   **Good practices** for experimenting.\n",
    "*   Intro to **transformers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4-E45fvrp3Z"
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First of all, we need to import some useful packages that we will use during this hands-on session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUXZLYya69wc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# system packages\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# data and numerical management packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# useful during debugging (progress bars)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# typing\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qImKj2Mu7LCX"
   },
   "source": [
    "### 1.1 Prepare a dataset for experiments\n",
    "\n",
    "We will use the IMDB dataset of previous tutorial.\n",
    "\n",
    "* [**Stats**] A dataset of 50k sentences used for sentiment analysis: 25k with positive sentiment, 25k with negative one.\n",
    "* [**Temporary**] We ignore sentiment labels since we focus on learning a proper word embedding representation.\n",
    "\n",
    "We start by **downloading** the dataset and **extract** it to a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSvqBcKJ7iTY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import tarfile\n",
    "\n",
    "print(f\"Current work directory: {os.getcwd()}\")\n",
    "dataset_folder = os.path.join(os.getcwd(), \"Datasets\")\n",
    "\n",
    "if not os.path.exists(dataset_folder):\n",
    "    os.makedirs(dataset_folder)\n",
    "\n",
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset_path = os.path.join(dataset_folder, \"Movies.tar.gz\")\n",
    "print(dataset_path)\n",
    "\n",
    "def download_dataset(download_path: str, url: str):\n",
    "    if not os.path.exists(download_path):\n",
    "        print(\"Downloading dataset...\")\n",
    "        request.urlretrieve(url, download_path)\n",
    "        print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: str, extract_path: str):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with tarfile.open(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(extract_path)\n",
    "    print(\"Extraction completed!\")\n",
    "\n",
    "\n",
    "download_dataset(dataset_path, url)\n",
    "extract_dataset(dataset_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3FVyXxr7llG"
   },
   "source": [
    "#### Data Inspection\n",
    "\n",
    "Feel free to check the dataset folder content!\n",
    "\n",
    "Usually, the README file is a good starting point (if it exists and it is informative -- which is not so common!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv3NW1SNrp3a"
   },
   "source": [
    "#### Data Format\n",
    "\n",
    "Just like in the first assignment, we need a **high level view** of the dataset that is helpful to our needs. \n",
    "\n",
    "We encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P05YfYCe7qCj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_dataset(dataset_name: str) -> pd.DataFrame:\n",
    "    dataframe_rows = []\n",
    "    for split in tqdm(['train', 'test']):\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            folder = os.path.join(os.getcwd(), \"Datasets\", dataset_name, split, sentiment)\n",
    "            for filename in os.listdir(folder):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        with open(file_path, mode='r', encoding='utf-8') as text_file:\n",
    "                            # read it and extract \n",
    "                            text = text_file.read()\n",
    "                            score = filename.split(\"_\")[1].split(\".\")[0]\n",
    "                            file_id = filename.split(\"_\")[0]\n",
    "                            num_sentiment = -1\n",
    "                            if sentiment == \"pos\" : num_sentiment = 1\n",
    "                            elif sentiment == \"neg\" : num_sentiment = 0\n",
    "\n",
    "                            # create single dataframe row\n",
    "                            dataframe_row = {\n",
    "                                \"file_id\": file_id,\n",
    "                                \"score\": score,\n",
    "                                \"sentiment\": num_sentiment,\n",
    "                                \"split\": split,\n",
    "                                \"text\": text\n",
    "                            }\n",
    "                            dataframe_rows.append(dataframe_row)\n",
    "                except Exception as e:\n",
    "                    print('Failed to process %s. Reason: %s' % (file_path, e))\n",
    "                    sys.exit(0)\n",
    "\n",
    "    folder = os.path.join(os.getcwd(), \"Datasets\", \"Dataframes\", dataset_name)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # transform the list of rows in a proper dataframe\n",
    "    df = pd.DataFrame(dataframe_rows)\n",
    "    df = df[[\"file_id\", \"score\", \"sentiment\", \"split\", \"text\"]]\n",
    "    dataframe_path = os.path.join(folder, dataset_name + \".pkl\")\n",
    "    df.to_pickle(dataframe_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIy1WQdirp3b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Encoding dataset...\")\n",
    "df = encode_dataset(dataset_name='aclImdb')\n",
    "print(\"Encoding completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va6D4sBv74mF"
   },
   "source": [
    "Let's inspect some major details of the dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3a3hqAP7-6r",
    "outputId": "65102475-2651-464d-89f2-e56051f92866",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (50000, 5)\n",
      "Dataset columns: ['file_id' 'score' 'sentiment' 'split' 'text']\n",
      "Classes distribution:\n",
      "1    25000\n",
      "0    25000\n",
      "Name: sentiment, dtype: int64\n",
      "Some examples:   file_id score  sentiment  split  \\\n",
      "0    8966    10          1  train   \n",
      "1     218     9          1  train   \n",
      "2    2585     7          1  train   \n",
      "3   12079     7          1  train   \n",
      "4    7901     7          1  train   \n",
      "\n",
      "                                                text  \n",
      "0  this is a wonderful film, makes the 1950'S loo...  \n",
      "1  Taking over roles that Jack Albertson and Sam ...  \n",
      "2  Here's a rare gem for those of you that haven'...  \n",
      "3  For those who think of Dame May Witty as the k...  \n",
      "4  Worst movie of all time? Wow, whoa now. You ca...  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset size: {df.shape}\") # (50000, 5)\n",
    "print(f\"Dataset columns: {df.columns.values}\") # ['file_id', 'score', 'sentiment', 'split', 'text]\n",
    "print(f\"Classes distribution:\\n{df.sentiment.value_counts()}\") # [0: 25000, 1: 25000]\n",
    "print(f\"Some examples: {df.iloc[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjf3k-qVrp3b"
   },
   "source": [
    "# PART I\n",
    "*   Building a **vocabulary**.\n",
    "*   Building a **word-word co-occurrence matrix**.\n",
    "*   Defining a **similarity metric**: cosine similarity.\n",
    "*   Embedding **visualization** and **analysis** of their semantic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sz1WJqE6nVg"
   },
   "source": [
    "## 1. Building a vocabulary\n",
    "\n",
    "We consider words as the atomic units for text representation: each word will be associated with a numeric representation.\n",
    "\n",
    "At this point we can build the word vocabulary of our dataset. This information is the first step of any word embedding method: we need to know the set of atomic entities that build up our corpus.\n",
    "\n",
    "**Definition**: a vocabulary is a collection of words occurring in a given dataset. More precisely, each word is recognized and assigned to an index.\n",
    "\n",
    "**Example**: Suppose you have the given toy corpus $D$: { \"the cat is on the table\" }\n",
    "\n",
    "As you notice, the dataset is comprised of only one sentence: \"the cat is on the table\". The corresponding vocabulary (a possible one) will be:\n",
    "\n",
    "V = {0: 'the', 1: 'cat', 2: 'is', 3: 'on', 4: 'table'}\n",
    "\n",
    "In this case, indexing follows word order, but it is not mandatory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5qeqzylrp3b"
   },
   "source": [
    "### Note 1\n",
    "The most important thing to remember is that the vocabulary should always be the same one. <br>\n",
    "$\\rightarrow$ Thus, make sure that the vocabulary creation routine always returns the same result!\n",
    "\n",
    "### Note 2\n",
    "A vocabulary is exclusively defined by the tokenization step you define! <br>\n",
    "$\\rightarrow$ Characters, sub-words, words are examples of granularity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J59S-tMg8a1X"
   },
   "source": [
    "### 1.1 Text pre-processing\n",
    "\n",
    "Before vocabulary creation, we have to do a little bit of **text pre-processing** so as to avoid spurious data. <br>\n",
    "$\\rightarrow$ Data quality is one of the crucial factors that lead to better performance. <br>\n",
    "$\\rightarrow$ Models, even state-of-the-art ones, hardly achieve satisfying results if the dataset is very noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjD3T_Nnrp3c"
   },
   "source": [
    "**Types of pre-processing**: there are a lot of pre-processing steps that we can consider, either general or quite task- specific. <br>\n",
    "Here we will rely on very standard and simple methods.\n",
    "\n",
    "*    **Text to lower**: casing usually doesn't affect our task, but in some scenarios, such as part-of-speech tagging, might even be crucial.\n",
    "\n",
    "*    **Replace special characters**: special characters are usually employed as variants of a single character like the spacing symbol ' '. \n",
    "In other cases (dates, etc..) special characters might have a specific meaning and should not be replaced.\n",
    "\n",
    "*    **Text stripping**: it is important to filter out extra spaces to avoid unwanted distinctions between identical words, such as 'apple' and ' apple '.\n",
    "\n",
    "There are a lot of pre-processing techniques, such as number replacing, lemmatization, stemming, spell correction, acronyms merge and so on. <br>\n",
    "$\\rightarrow$ If you are interested you can check [here](https://medium.com/swlh/text-normalization-7ecc8e084e31) and [here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) some good blogs about the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uCREu9urp3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "GOOD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "try:\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7HqhEjl8cqg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lower(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Transforms given text to lower case.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def replace_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces special characters, such as paranthesis, with spacing character\n",
    "    \"\"\"\n",
    "    return REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "\n",
    "def replace_br(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces br characters\n",
    "    \"\"\"\n",
    "    return text.replace('br', '')\n",
    "\n",
    "def filter_out_uncommon_symbols(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any special character that is not in the good symbols list (check regular expression)\n",
    "    \"\"\"\n",
    "    return GOOD_SYMBOLS_RE.sub('', text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    return ' '.join([x for x in text.split() if x and x not in STOPWORDS])\n",
    "\n",
    "def strip_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any left or right spacing (including carriage return) from text.\n",
    "    \"\"\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bePDQapQrp3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PREPROCESSING_PIPELINE = [\n",
    "                          lower,\n",
    "                          replace_special_characters,\n",
    "                          replace_br,\n",
    "                          filter_out_uncommon_symbols,\n",
    "                          remove_stopwords,\n",
    "                          strip_text\n",
    "                          ]\n",
    "\n",
    "def text_prepare(text: str,\n",
    "                 filter_methods: List[Callable[[str], str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Applies a list of pre-processing functions in sequence (reduce).\n",
    "    Note that the order is important here!\n",
    "    \"\"\"\n",
    "    filter_methods = filter_methods if filter_methods is not None else PREPROCESSING_PIPELINE\n",
    "    return reduce(lambda txt, f: f(txt), filter_methods, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36RFsheCrp3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Pre-processing text...')\n",
    "\n",
    "print()\n",
    "print(f'[Debug] Before:\\n{df.text.values[:2]}')\n",
    "print()\n",
    "\n",
    "# Replace each sentence with its pre-processed version\n",
    "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))\n",
    "\n",
    "print(f'[Debug] After:\\n{df.text.values[:2]}')\n",
    "print()\n",
    "\n",
    "print(\"Pre-processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk_mzom18yLV"
   },
   "source": [
    "### 1.2 Vocabulary Creation\n",
    "\n",
    "Since the text has been pre-processed, space splitting should work correctly. <br>\n",
    "We proceed on building the vocabulary and perform some sanity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37jKjW3arp3c"
   },
   "source": [
    "$\\rightarrow$ Bare in mind that some packages offers tools for automatic vocabulary creation, such as Keras (check [keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)). \n",
    "\n",
    "**Note**: In this case, the vocabulary will start from index equal to 1, since the 0 slot is reserved to padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjhjcZKg82-U",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def build_vocabulary(df: pd.DataFrame) -> (Dict[int, str],\n",
    "                                           Dict[str, int],\n",
    "                                           List[str]):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary.\n",
    "\n",
    "    :param df: dataset from which we want to build the word vocabulary (pandas.DataFrame)\n",
    "    :return:\n",
    "      - word vocabulary: vocabulary index to word\n",
    "      - inverse word vocabulary: word to vocabulary index\n",
    "      - word listing: set of unique terms that build up the vocabulary\n",
    "    \"\"\"\n",
    "    idx_to_word = OrderedDict()\n",
    "    word_to_idx = OrderedDict()\n",
    "    \n",
    "    curr_idx = 0\n",
    "    for sentence in tqdm(df.text.values):\n",
    "        tokens = sentence.split()\n",
    "        for token in tokens:\n",
    "            if token not in word_to_idx:\n",
    "                word_to_idx[token] = curr_idx\n",
    "                idx_to_word[curr_idx] = token\n",
    "                curr_idx += 1\n",
    "\n",
    "    word_listing = list(idx_to_word.values())\n",
    "    return idx_to_word, word_to_idx, word_listing\n",
    " \n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(df)\n",
    "print(f'[Debug] Index -> Word vocabulary size: {len(idx_to_word)}')\n",
    "print(f'[Debug] Word -> Index vocabulary size: {len(word_to_idx)}')\n",
    "print(f'[Debug] Some words: {[(idx_to_word[idx], idx) for idx in np.arange(10) + 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqY9afATrp3d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_vocabulary(idx_to_word: Dict[int, str], word_to_idx: Dict[str, int],\n",
    "                        word_listing: List[str], df: pd.DataFrame, check_default_size: bool = False):\n",
    "    print(\"[Vocabulary Evaluation] Size checking...\")\n",
    "    assert len(idx_to_word) == len(word_to_idx)\n",
    "    assert len(idx_to_word) == len(word_listing)\n",
    "\n",
    "    print(\"[Vocabulary Evaluation] Content checking...\")\n",
    "    for i in tqdm(range(0, len(idx_to_word))):\n",
    "        assert idx_to_word[i] in word_to_idx\n",
    "        assert word_to_idx[idx_to_word[i]] == i\n",
    "\n",
    "    print(\"[Vocabulary Evaluation] Consistency checking...\")\n",
    "    _, _, first_word_listing = build_vocabulary(df)\n",
    "    _, _, second_word_listing = build_vocabulary(df)\n",
    "    assert first_word_listing == second_word_listing\n",
    "\n",
    "    print(\"[Vocabulary Evaluation] Toy example checking...\")\n",
    "    toy_df = pd.DataFrame.from_dict({\n",
    "        'text': [\"all that glitters is not gold\", \"all in all i like this assignment\"]\n",
    "    })\n",
    "    _, _, toy_word_listing = build_vocabulary(toy_df)\n",
    "    toy_valid_vocabulary = set(' '.join(toy_df.text.values).split())\n",
    "    assert set(toy_word_listing) == toy_valid_vocabulary\n",
    "\n",
    "print(\"Vocabulary evaluation...\")\n",
    "evaluate_vocabulary(idx_to_word, word_to_idx, word_listing, df)\n",
    "print(\"Evaluation completed!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoSJR73EF2Tf"
   },
   "source": [
    "### Note\n",
    "Define **intermediary tests** for your code in order to inspect data and to assess the correctness of your code! <br>\n",
    "$\\rightarrow$ You don't want to **re-run** huge and time-consuming experiments due to early pipeline errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AnQ4bvP86-w"
   },
   "source": [
    "### 1.3 Saving the vocabulary\n",
    "\n",
    "Generally speaking, it is a good idea to save the dictionary in clear format. <br>\n",
    "$\\rightarrow$ In this way you can quickly check for errors or useful words.\n",
    "\n",
    "In this case, we will save the vocabulary dictionary in **JSON format**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i51oflz4WVI"
   },
   "outputs": [],
   "source": [
    "!pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlPOkSbv8_Fv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import simplejson as sj\n",
    "\n",
    "vocab_path = os.path.join(os.getcwd(), 'Datasets', \"aclImdb\", 'vocab.json')\n",
    "\n",
    "print(f\"Saving vocabulary to {vocab_path}\")\n",
    "with open(vocab_path, mode='w') as f:\n",
    "    sj.dump(word_to_idx, f, indent=4)\n",
    "print(\"Saving completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2POKdKl7A28"
   },
   "source": [
    "## 2. Sparse embeddings\n",
    "\n",
    "Working with text inherently requires a numerical conversion step, known as **embedding**.\n",
    "\n",
    "**Bag-of-Words (BoW)**\n",
    "1.   Count the occurrence of each word in a given corpus\n",
    "2.   Build a word co-occurrence matrix: useful to identify the most common terms in each given document.\n",
    "\n",
    "This type of reasoning is directly related to **how meaning is assigned to words**. <br>\n",
    "$\\rightarrow$  In particular, it is the environment enclosing a word that gives a specific meaning to it. <br>\n",
    "$\\rightarrow$  Thus, we look for numerical encoding methods that reflect such point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5lllkXW8MN-"
   },
   "source": [
    "### 2.1 A quick simplification\n",
    "\n",
    "Since the dataset is quite large, the co-occurrence matrix construction may take a while or may require efficient solutions. <br>\n",
    "\n",
    "For the purpose of this tutorial, we can rely on a **small slice** of the dataset. <br>\n",
    "$\\rightarrow$ Nonetheless, feel free to work with the whole dataset! Suggestions on how to handle this scenario are\n",
    "given below when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muKUp_f_8VYz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This type of slicing is not mandatory, but it is sufficient to our purposes\n",
    "np.random.seed(42)\n",
    "random_indexes = np.random.choice(np.arange(df.shape[0]),\n",
    "                                  size=1000,\n",
    "                                  replace=False)\n",
    "\n",
    "df = df.iloc[random_indexes]\n",
    "print(f'New dataset size: {df.shape}')\n",
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7RNPP1N9FxD"
   },
   "source": [
    "### 2.2 Building the Co-occurence Matrix\n",
    "\n",
    "For each word in the vocabulary we count the number of times each other word appears within the same context window. A simple example is given by image below.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1UknGoYvIBBA7ytkSlqm1NhF_lHt0iOwT)\n",
    "\n",
    "Let's define the simplest version of a **co-occurrence matrix** based on word counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbuS9RBprp3d"
   },
   "source": [
    "### Small dataset case\n",
    "You should have a vocabulary size that we can afford in terms of memory demand. <br>\n",
    "$\\rightarrow$ You can easily instantiate the co-occurrence matrix and populate it iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phOfKV_8rp3d"
   },
   "source": [
    "### Large dataset case\n",
    "We have to work with sparse matrices due to the high vocabulary size and to the low amount of non-zero word counts. <br>\n",
    "$\\rightarrow$ The [Scipy package](https://docs.scipy.org/doc/scipy/reference/sparse.html) allows us to easily define sparse matrices that can be converted ot numpy arrays.\n",
    "\n",
    "**Suggestion**: The simplest way to build the co-occurrence matrix is via an incremental approach:\n",
    "1. Loop through dataset sentences\n",
    "2. Split into words\n",
    "3. Count co-occurrences within the given window frame. \n",
    "\n",
    "Combining this approach with sparse matrices is not so efficient (yet possible). However, Scipy offers [$\\texttt{lil_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html#scipy.sparse.lil_matrix) sparse format that is suitable to this case. \n",
    "\n",
    "You can check out other sparse formats, such as [$\\texttt{csr_matrix}$](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix), and the corresponding building methods.\n",
    "\n",
    "Working with $\\texttt{lil_matrix}$ might take $\\sim 1h$ of time to build the whole dataset co-occurrence matrix. It is also possibile to work with $\\texttt{csr_matrix}$ but the approach is more complex (check the last example of the corresponding documentation page).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0g9enjW9IuZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import gc\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def co_occurrence_count(df: pd.DataFrame,\n",
    "                        idx_to_word: Dict[int, str],\n",
    "                        word_to_idx: Dict[str, int],\n",
    "                        window_size: int = 4) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds word-word co-occurrence matrix based on word counts.\n",
    "\n",
    "    :param df: pre-processed dataset (pandas.DataFrame)\n",
    "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "\n",
    "    :return\n",
    "      - co_occurrence symmetric matrix of size |V| x |V| (|V| = vocabulary size)\n",
    "    \"\"\"\n",
    "    vocab_size = len(idx_to_word)\n",
    "    co_occurrence_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for sentence in tqdm(df.text.values):\n",
    "        tokens = sentence.split()\n",
    "        for pos, token in enumerate(tokens):\n",
    "            start = max(0, pos - window_size)\n",
    "            end = min(pos + window_size + 1, len(tokens))\n",
    "\n",
    "            first_word_index = word_to_idx[token]\n",
    "\n",
    "            for pos2 in range(start, end):\n",
    "                if pos2 != pos:\n",
    "                    second_token = tokens[pos2]\n",
    "                    second_word_index = word_to_idx[second_token]\n",
    "                    co_occurrence_matrix[first_word_index, second_word_index] += 1\n",
    "\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dkzkF2Rrp3d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean RAM before re-running this code snippet to avoid possible session crash\n",
    "if 'co_occurrence_matrix' in globals():\n",
    "    del co_occurrence_matrix\n",
    "    gc.collect()\n",
    "    time.sleep(10.)     # Give colab some time \n",
    "\n",
    "print(\"Building co-occurrence count matrix... (it may take a while...)\")\n",
    "co_occurrence_matrix = co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4)\n",
    "print(\"Building completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDXIClMH9kcO"
   },
   "source": [
    "### 2.3 Embedding Visualization\n",
    "\n",
    "The next step is to visualize our sparse word embeddings in a lower dimensional space (2D) in order to have an idea of the meaning attributed to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pdh0-RJQrp3e"
   },
   "source": [
    "**How?** We will explore SVD and t-SNE methods, without delving into technical details since they are not arguments of this NLP course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-zCGK6-rp3e"
   },
   "source": [
    "**SVD Memo**: SVD stands for **Singular Value Decomposition** and is a kind of generalized **Principal Components Analysis** (PCA) and focuses on selecting the top **k** principal components. For more info, [here](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf) you can find a brief tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM91KE6qrp3e"
   },
   "source": [
    "**t-SNE Memo**: t-SNE stands for **t-Distributed Stochastic Neighbour Embedding** and is an unsupervised non-linear technique.\n",
    "* It preserves small pairwise distance (or local similarities), whereas PCA aims to preserve large pairwise distances in order to maximize variance. \n",
    "* The basic idea of t-SNE is to compute a similarity measure between a pair of instances both at high and low dimensional space and optimize these two similarities via a cost function. \n",
    "* Properly using t-SNE is a bit tricky, a well recommended reading is one of the [author's blog](https://lvdmaaten.github.io/tsne/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JFFdOberp3e"
   },
   "source": [
    "**UMAP Memo**: UMAP stands for **Uniform Manifold Approximation and Projection for Dimensionality Reduction** and is an unsupervised non-linear technique like t-SNE.\n",
    "\n",
    "* **Faster** than t-SNE\n",
    "* **More accurate** than t-SNE in terms of data's global structure preservation\n",
    "* Informally constructs a high-dimensional graph representation of the data and then optimizes a low-dimensional graph to be as structurally similar as possibile\n",
    "* Check this [blog](https://pair-code.github.io/understanding-umap/) if you are interested!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnAZSVoIrp3e"
   },
   "source": [
    "**Note**: We **strongly suggest you** to play with the window size and check if there are some notable differences. <br>\n",
    "Generally:\n",
    "* Small window size $\\rightarrow$ reflects syntactic properties.\n",
    "* Large window size $\\rightarrow$ captures semantic ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVFSLAiW4a64"
   },
   "outputs": [],
   "source": [
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z38cC84Krp3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6gpCg8n9mw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings: np.ndarray,\n",
    "                         word_annotations: List[str] = None,\n",
    "                         word_to_idx: Dict[str, int] = None):\n",
    "    \"\"\"\n",
    "    Plots given reduce word embeddings (2D). Users can highlight specific words (word_annotations list).\n",
    "\n",
    "    :param embeddings: word embedding matrix of shape (words, 2) retrieved via a dimensionality reduction technique.\n",
    "    :param word_annotations: list of words to be annotated.\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "\n",
    "    if word_annotations:\n",
    "        print(f\"Annotating words: {word_annotations}\")\n",
    "\n",
    "        word_indexes = []\n",
    "        for word in word_annotations:\n",
    "            word_index = word_to_idx[word]\n",
    "            word_indexes.append(word_index)\n",
    "\n",
    "        word_indexes = np.array(word_indexes)\n",
    "\n",
    "        other_embeddings = embeddings[np.setdiff1d(np.arange(embeddings.shape[0]), word_indexes)]\n",
    "        target_embeddings = embeddings[word_indexes]\n",
    "\n",
    "        ax.scatter(other_embeddings[:, 0], other_embeddings[:, 1], alpha=0.1, c='blue')\n",
    "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1.0, c='red')\n",
    "        ax.scatter(target_embeddings[:, 0], target_embeddings[:, 1], alpha=1, facecolors='none', edgecolors='r', s=1000)\n",
    "\n",
    "        for word, word_index in zip(word_annotations, word_indexes):\n",
    "            word_x, word_y = embeddings[word_index, 0], embeddings[word_index, 1]\n",
    "            ax.annotate(word, xy=(word_x, word_y))\n",
    "    else:\n",
    "        ax.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.1, c='blue')\n",
    "\n",
    "    # We avoid outliers ruining the visualization if they are quite far away\n",
    "    xmin_quantile = np.quantile(embeddings[:, 0], q=0.01)\n",
    "    xmax_quantile = np.quantile(embeddings[:, 0], q=0.99)\n",
    "    ymin_quantile = np.quantile(embeddings[:, 1], q=0.01)\n",
    "    ymax_quantile = np.quantile(embeddings[:, 1], q=0.99)\n",
    "    ax.set_xlim(xmin_quantile, xmax_quantile)\n",
    "    ax.set_ylim(ymin_quantile, ymax_quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYWb_U-Drp3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce_SVD(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies SVD dimensionality reduction.\n",
    "\n",
    "    :param embeddings: word embedding matrix of shape (words, dim). In the case\n",
    "                       of a word-word co-occurrence matrix the matrix shape would\n",
    "                       be (words, words).\n",
    "\n",
    "    :return\n",
    "        - 2-dimensional word embedding matrix of shape (words, 2)\n",
    "    \"\"\"\n",
    "    print(\"Running SVD reduction method...\")\n",
    "    svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
    "    reduced = svd.fit_transform(embeddings)\n",
    "    print(\"SVD reduction completed!\")\n",
    "\n",
    "    return reduced\n",
    "\n",
    "def reduce_tSNE(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies t-SNE dimensionality reduction.\n",
    "    \"\"\"\n",
    "    print(\"Running t-SNE reduction method... (it may take a while...)\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, n_iter=1000,\n",
    "                metric='cosine', n_jobs=4)\n",
    "    reduced = tsne.fit_transform(embeddings)\n",
    "    print(\"t-SNE reduction completed!\")\n",
    "\n",
    "    return reduced\n",
    "\n",
    "def reduce_umap(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies UMAP dimensionality reduction.\n",
    "    \"\"\"\n",
    "    print(\"Running UMAP reduction method... (it may take a while...)\")\n",
    "    umap_emb = umap.UMAP(n_components=2, random_state=42, metric='cosine', n_jobs=4)\n",
    "    reduced = umap_emb.fit_transform(embeddings)\n",
    "    print(\"UMAP reduction completed!\")\n",
    "    \n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rASm9X4rp3e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feel free to play with word_annotations argument!\n",
    "# Check the saved dictionary!\n",
    "def run_visualization(method_name: str,\n",
    "                      words_list: List[str],\n",
    "                      word_to_idx: Dict[str, int],\n",
    "                      co_occurrence_matrix):\n",
    "    method_name = method_name.lower().strip()\n",
    "    method_map = {\n",
    "        'svd': reduce_SVD,\n",
    "        'tsne': reduce_tSNE,\n",
    "        'umap': reduce_umap\n",
    "    }\n",
    "    \n",
    "    if method_name not in method_map:\n",
    "        raise RuntimeError(f'Invalid method name! Got {method_name}.')\n",
    "    \n",
    "    reduced = method_map[method_name](co_occurrence_matrix)\n",
    "    visualize_embeddings(reduced, words_list, word_to_idx)\n",
    "        \n",
    "\n",
    "# SVD\n",
    "# run_visualization('svd', ['good', 'love', 'beautiful'], word_to_idx, co_occurrence_matrix)\n",
    "\n",
    "# t-SNE\n",
    "# run_visualization('tsne', ['good', 'love', 'beautiful'], word_to_idx, co_occurrence_matrix)\n",
    "\n",
    "# UMAP\n",
    "run_visualization('umap', ['good', 'love', 'beautiful'], word_to_idx, co_occurrence_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8R1DwJg9-L9"
   },
   "source": [
    "### 2.4 Embedding properties\n",
    "\n",
    "Visualization can give us a rough idea of how word embeddings are organized and if some semantic properties are reflected in the numerical dimensional space. <br>\n",
    "$\\rightarrow$ For example, are synonyms close together? Ideally, if the dataset is big enough, we should see similar vector embeddings since synonyms usually have similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReeTvqkCrp3e"
   },
   "source": [
    "##### How to do that?\n",
    "We could highlight target words in the visualization step and check if our expectations are met. For instance, synonyms should be close together. <br>\n",
    "\n",
    "**However**, this method is rather **inaccurate** and **time-consuming** (dimensionality reduction is not a perfect mapping). <br>\n",
    "$\\rightarrow$ We need some sort of similarity metric that is independent of the vector dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7e6iVYA-BZk"
   },
   "source": [
    "### 2.5 Cosine Similarity\n",
    "\n",
    "We want to measure how two word vectors are far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DGpClSWrp3f"
   },
   "source": [
    "#### Naive Method\n",
    "\n",
    "A naive solution would involve computing the dot product of the two vectors. However, this metric will give higher similarity either to longer vectors or to vectors that have higher counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQEMlf5-rp3f"
   },
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "A better metric is **cosine similarity** which is just a normalized dot product.\n",
    "\n",
    "\\begin{align}\n",
    "s(p, q) = \\frac{p \\, \\cdot \\, q}{||p|| \\, \\cdot \\, ||q||}\n",
    "\\end{align}\n",
    "\n",
    "where $s(p, q) \\in [-1, 1] $, since it computes the cosine of the angle between the two vectors. \n",
    "\n",
    "Intuitively, we are bringing vectors down to the d-dimensional unit sphere (d is the vocab size) and then computing their distance (in 2D space we will have a circle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eXzwsPw-Fko",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as sk_cs\n",
    "\n",
    "def cosine_similarity(p: np.ndarray,\n",
    "                      q: np.ndarray,\n",
    "                      transpose_p: bool = False,\n",
    "                      transpose_q: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity of two d-dimensional matrices\n",
    "\n",
    "    :param p: d-dimensional vector (np.ndarray) of shape (p_samples, d)\n",
    "    :param q: d-dimensional vector (np.ndarray) of shape (q_samples, d)\n",
    "    :param transpose_p: whether to transpose p or not\n",
    "    :param transpose_q: whether to transpose q or not\n",
    "\n",
    "    :return\n",
    "        - cosine similarity matrix S of shape (p_samples, q_samples)\n",
    "          where S[i, j] = s(p[i], q[j])\n",
    "    \"\"\"\n",
    "    if len(p.shape) == 1:\n",
    "        p = p.reshape(1, -1)\n",
    "    if len(q.shape) == 1:\n",
    "        q = q.reshape(1, -1)\n",
    "\n",
    "    return sk_cs(p, q)\n",
    "\n",
    "similarity_matrix = cosine_similarity(co_occurrence_matrix,\n",
    "                                      co_occurrence_matrix,\n",
    "                                      transpose_q=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd1Itvn5-h12"
   },
   "source": [
    "### 2.6 [Let's play] Analogies, Bias, Synonyms and Antonyms\n",
    "\n",
    "Let's look for some words and provide a possible explanation of achieved results according to cosine similarity metric.\n",
    "\n",
    "* Synonym pair: (w1, w2) such that w1 and w2 are synonyms\n",
    "* Antonyms pair: (w1, w2) such that w1 and w2 are antonyms\n",
    "* Synonym-Antonym triplet: (w1, w2, w3) such that w1 and w2 are synonyms and w1 and w3 are antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ti71XDdrp3f"
   },
   "source": [
    "#### Analogy\n",
    "\n",
    "Another useful property to check is analogy resolution via word vectors. \n",
    "\n",
    "In particular, we might want to check if analogies such \"man : king == woman : x\" bring results like \"x = queen\".\n",
    "\n",
    "In order to do so, we first need to define a ranking function that returns the top $K$ most similar words of a given word vector. <br>\n",
    "$\\rightarrow$ We might not want to be too much restrictive and play with $K \\ge 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NV4ZW-zS4WFK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def show_similarity_for_nary(words: List[str],\n",
    "                             similarity_matrix: np.ndarray,\n",
    "                             word_to_idx: Dict[str, int],\n",
    "                             idx_to_word: Dict[int, str]):\n",
    "    \"\"\"\n",
    "    Shows similarity values for each pair of input words.\n",
    "\n",
    "    :param words: a list of candidate words.\n",
    "    :param similarity_matrix: np.ndarray containing similarity values \n",
    "    between each vocabulary word pair\n",
    "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "\n",
    "    \"\"\"\n",
    "    word_indexes = [word_to_idx[word] for word in words]\n",
    "    similarity_dict = {}\n",
    "\n",
    "    for comb in product(word_indexes, word_indexes):\n",
    "        similarity_value = similarity_matrix[comb[0], comb[1]]\n",
    "        similarity_dict.setdefault(comb[0], []).append(similarity_value)\n",
    "\n",
    "    similarity_df = pd.DataFrame.from_dict(similarity_dict)\n",
    "    similarity_df.columns = [idx_to_word[col] for col in similarity_df.columns]\n",
    "    similarity_df.index = similarity_df.columns\n",
    "    similarity_df = similarity_df.transpose()\n",
    "    print(F'Similarity values: \\n{similarity_df}')\n",
    "\n",
    "show_similarity_for_nary(['film', 'movie'], similarity_matrix, word_to_idx, idx_to_word)\n",
    "show_similarity_for_nary(['good', 'bad'], similarity_matrix, word_to_idx, idx_to_word)\n",
    "show_similarity_for_nary(['good', 'well', 'bad'], similarity_matrix, word_to_idx, idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzVmI7eR-lak",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top_K_indexes(data: np.ndarray, K: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns the top K indexes of a 1-dimensional array (descending order)\n",
    "    Example:\n",
    "        data = [0, 7, 2, 1]\n",
    "        best_indexes:\n",
    "        K = 1 -> [1] (data[1] = 7)\n",
    "        K = 2 -> [1, 2]\n",
    "        K = 3 -> [1, 2, 3]\n",
    "        K = 4 -> [1, 2, 3, 4]\n",
    "\n",
    "    :param data: 1-d dimensional array\n",
    "    :param K: number of highest value elements to consider\n",
    "\n",
    "    :return\n",
    "        - array of indexes corresponding to elements of highest value\n",
    "    \"\"\"\n",
    "    best_indexes = np.argsort(data, axis=0)[::-1]\n",
    "    best_indexes = best_indexes[:K]\n",
    "\n",
    "    return best_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcCFE_BCrp3f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_top_K_word_ranking(embedding_matrix: np.ndarray, idx_to_word: Dict[int, str],\n",
    "                           word_to_idx: Dict[str, int],  positive_listing: List[str],\n",
    "                           negative_listing: List[str],  K: int) -> (List[str], np.ndarray):\n",
    "    \"\"\"\n",
    "    Finds the top K most similar words following this reasoning:\n",
    "        1. words that have highest similarity to words in positive_listing\n",
    "        2. words that have highest distance to words in negative_listing\n",
    "        \n",
    "    :param embedding_matrix: embedding matrix of shape (words, embedding dimension).\n",
    "    :param idx_to_word: vocabulary map (index -> word) (dict)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param positive_listing: list of words that should have high similarity with top K retrieved ones.\n",
    "    :param negative_listing: list of words that should have high distance to top K retrieved ones.\n",
    "    :param K: number of best word matches to consider\n",
    "\n",
    "    :return\n",
    "        - top K word matches according to aforementioned criterium\n",
    "        - similarity values of top K word matches according to aforementioned criterium\n",
    "    \"\"\"\n",
    "    # Positive words (similarity)\n",
    "    positive_indexes = np.array([word_to_idx[word] for word in positive_listing])\n",
    "    word_positive_vector = np.sum(embedding_matrix[positive_indexes, :], axis=0)\n",
    "\n",
    "    # Negative words (distance)\n",
    "    negative_indexes = np.array([word_to_idx[word] for word in negative_listing])\n",
    "    word_negative_vector = np.sum(embedding_matrix[negative_indexes, :], axis=0)\n",
    "\n",
    "    # Find candidate words\n",
    "    target_vector = (word_positive_vector - word_negative_vector) / (len(positive_listing) + len(negative_listing))\n",
    "    total_indexes = np.concatenate((positive_indexes, negative_indexes))\n",
    "    valid_indexes = np.setdiff1d(np.arange(similarity_matrix.shape[0]), total_indexes)\n",
    "    candidate_vectors = embedding_matrix[valid_indexes]\n",
    "\n",
    "    candidate_similarities = cosine_similarity(candidate_vectors, target_vector, transpose_q=True)\n",
    "    candidate_similarities = candidate_similarities.ravel()\n",
    "\n",
    "    relative_indexes = get_top_K_indexes(candidate_similarities, K)\n",
    "    top_K_indexes = valid_indexes[relative_indexes]\n",
    "    top_K_words = [idx_to_word[idx] for idx in top_K_indexes]\n",
    "\n",
    "    return top_K_words, candidate_similarities[relative_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ui0Qbn2L-rM8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Positive and negative listing can be defined accordingly to a given analogy\n",
    "    Example:\n",
    "        \n",
    "        man : king :: woman : x\n",
    "    \n",
    "    positive_listing = ['king', 'woman']\n",
    "    negative_listing = ['man']\n",
    "\n",
    "    This is equivalent to: compute king - man + woman, and then find the\n",
    "    most similar candidate.\n",
    "\"\"\"\n",
    "\n",
    "# Examples\n",
    "# tv : episodes :: film : x\n",
    "# masterpiece : superb :: x : tragic\n",
    "top_K_words, top_K_values = get_top_K_word_ranking(embedding_matrix=co_occurrence_matrix,\n",
    "                                                   idx_to_word=idx_to_word,\n",
    "                                                   word_to_idx=word_to_idx,\n",
    "                                                   positive_listing=['episodes', 'film'],\n",
    "                                                   negative_listing=['tv'],\n",
    "                                                   K=10)\n",
    "print(f'Top K words: {top_K_words}')\n",
    "print(f'Top K values: {top_K_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpohtztJrp3f"
   },
   "source": [
    "# PART II\n",
    "\n",
    "*   Loading pre-trained **dense** word embeddings: Word2Vec, GloVe, FastText.\n",
    "*   Checking **out-of-vocabulary** (OOV) terms.\n",
    "*   **Handling** OOV terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKaAlvqhFaSE"
   },
   "source": [
    "## 1. Dense embeddings\n",
    "\n",
    "Until now we've worked with sparse embedding methods, which lead to high dimensional word embeddings (dimension equal to $|V|$). \n",
    "\n",
    "The **main drawback** of such approach is that words belong to separate dimensions. <br>\n",
    "$\\rightarrow$ We need to have a large corpus available to check if two words have similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z3z2u6orp3g"
   },
   "source": [
    "#### Dense Embedding Technique\n",
    "\n",
    "To this end, we might prefer a dense embedding technique. <br>\n",
    "$\\rightarrow$ All words are encoded into a high dimensional space, much smaller than $|V|$ (generally up to $\\sim 1000$).\n",
    "\n",
    "A dense representation is also convenient from a machine learning point of view:\n",
    "*    **Fewer parameters** to learn and, thus, models are less prone to overfitting.\n",
    "*    Words do not belong to separate dimensions anymore and semantic relationships are easily modelled.\n",
    "\n",
    "In this section, we experiment with pre-trained dense embedding models and compare them to previously described sparse methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MpHvK4FFeap"
   },
   "source": [
    "### 1.1 Working with a pre-trained model\n",
    "\n",
    "The first step consists in choosing and downloading a pre-trained embedding model. \n",
    "\n",
    "For the purpose of this assignment, we limit to classic models, such as Word2Vec, GloVe and FastText.\n",
    "\n",
    "Furthermore, some pre-trained embedding model versions may be quite resource demanding, depending on the embedding dimension and on the vocabulary size. <br>\n",
    "$\\rightarrow$ We recommend sticking to low dimensional spaces (50, 100, 200) to avoid being stuck waiting for too much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3mQB6NNrp3g"
   },
   "source": [
    "### A Brief Recap\n",
    "\n",
    "* **Word2Vec**: the first example of dense word encoding. There are two well-known strategies:\n",
    "     * Continuous Bag-of-words (CBoW): context words are used to predict a target word.\n",
    "     * Skip-gram: a target word is used to predict its own context.\n",
    "* **GloVe**: it is another techniques that tries to encoded global semantic properties based on the co-occurrence matrix. Conversely, word2vec exploits local information.\n",
    "* **Fasttext**: an extension of word2vec where words are defined as character n-grams. It works very well with rare words contrarily to Word2Vec and GloVe.\n",
    "\n",
    "For a full list of available embeddings models, please check [here](https://github.com/RaRe-Technologies/gensim-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMd7o1cO4fxo"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIoWFwyAFmIg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(model_type: str,\n",
    "                         embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dimension: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    elif model_type.strip().lower() == 'fasttext':\n",
    "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        print('FastText: 300')\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUtONZmDrp3g",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modify these variables as you wish!\n",
    "# Glove -> 50, 100, 200, 300\n",
    "# Word2Vec -> 300\n",
    "# Fasttext -> 300\n",
    "embedding_model = load_embedding_model(model_type=\"glove\",\n",
    "                                       embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCsxb5g6Fnc_"
   },
   "source": [
    "### 1.2 Out of vocabulary (OOV) words\n",
    "\n",
    "Before evaluating pre-trained dense word embeddings, it is good practice to check if the model is consistent with our dataset. <br> \n",
    "$\\rightarrow$ We check the number of out-of-vocabulary (OOV) terms.\n",
    "\n",
    "If the OOV amount is negligible, we can just keep going. On the other hand, we might want to handle OOV terms by assigning them a specific word vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yi1Rq5cBrp3g"
   },
   "source": [
    "**Which one?** One common practice is to assign a **random vector**, since the embedding model will be part of a deep learning model and, thus, word vectors might be trained during the learning process. \n",
    "\n",
    "Even if that is the case, we can assign an embedding that is more meaningful rather than a random one. <br>\n",
    "$\\rightarrow$ We can identify the word embedding of an OOV term as the **mean of its neighbour word embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8s86gnQmFp-q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                    word_listing: List[str]):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms.\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_listing: dataset specific vocabulary (list)\n",
    "\n",
    "    :return\n",
    "        - list of OOV terms\n",
    "    \"\"\"\n",
    "    embedding_vocabulary = set(embedding_model.vocab.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpZzHiNMrp3g",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oov_terms = check_OOV_terms(embedding_model, word_listing)\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(word_listing)\n",
    "print(f\"Total OOV terms: {len(oov_terms)} ({oov_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrCrkTdiFsYp"
   },
   "source": [
    "### 1.3 Handling OOV words\n",
    "\n",
    "Now we proceed on building the embedding matrix, while handling OOV terms at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ks3PUZeyFuYB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx: Dict[str, int],\n",
    "                           vocab_size: int,\n",
    "                           oov_terms: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained word embedding model\n",
    "\n",
    "    :param embedding_model: pre-trained word embedding model (gensim wrapper)\n",
    "    :param word_to_idx: vocabulary map (word -> index) (dict)\n",
    "    :param vocab_size: size of the vocabulary\n",
    "    :param oov_terms: list of OOV terms (list)\n",
    "\n",
    "    :return\n",
    "        - embedding matrix that assigns a high dimensional vector to each word in the dataset specific vocabulary (shape |V| x d)\n",
    "    \"\"\"\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIheFWL0rp3g",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "embedding_dimension = 50\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word_to_idx, len(word_to_idx), oov_terms)\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT21zEwuFwbJ"
   },
   "source": [
    "### 1.4 Embedding visualization (cont'd)\n",
    "\n",
    "We are now ready to visualize pre-trained word embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU0xDlLfFzMc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UMAP\n",
    "reduced_embedding_umap = reduce_umap(embedding_matrix)\n",
    "visualize_embeddings(reduced_embedding_umap, ['good', 'love', 'beautiful'], word_to_idx)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gay1jSfwF2Qf"
   },
   "source": [
    "### 1.5 [Let's play!] Embedding properties (cont'd)\n",
    "\n",
    "Let's consider some previous examples for quick comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yMbNuIdF5s0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_K_words, top_K_values = get_top_K_word_ranking(embedding_matrix=embedding_matrix,\n",
    "                                                   idx_to_word=idx_to_word,\n",
    "                                                   word_to_idx=word_to_idx,\n",
    "                                                   positive_listing=['episodes', 'film'],\n",
    "                                                   negative_listing=['tv'],\n",
    "                                                   K=10)\n",
    "print(f'Top K words: {top_K_words}')\n",
    "print(f'Top K values: {top_K_values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7T-aPjErp3h"
   },
   "source": [
    "# PART III\n",
    "*   Problem setting: **sentiment analysis task**.\n",
    "*   Setting up a **data pipeline**: from dataset loading to data conversion.\n",
    "*   **Model definition** (with Keras framework).\n",
    "*   Model training, inference and **evaluation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNTJ4L7Frp3h"
   },
   "source": [
    "## 1. Problem setting\n",
    "\n",
    "In the second part of this tutorial, we'll tackle the same sentiment analysis task of Tutorial 1. <br>\n",
    "$\\rightarrow$ However, we'll consider simple neural network models as baselines.\n",
    "\n",
    "In particular, we'll briefly introduce:\n",
    "* How to properly **train** and **evaluate** neural networks.\n",
    "* Introduce some **regularization techniques** to avoid unwanted scenarios like model overfitting.\n",
    "* Good practices for **proper** experimental setup definition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8skS3zDirp3h"
   },
   "source": [
    "### 1.1 Task formulation\n",
    "\n",
    "As in Tutorial 1, we consider the classification task of predicting the sentiment of a given sentence. \n",
    "\n",
    "More precisely, we consider the binary classification problem where the sentiment label can either be $positive$ or $negative$.\n",
    "\n",
    "Formally, we denote:\n",
    "\n",
    "Input sentence --> $x$\n",
    "\n",
    "Sentiment label --> $y \\in [positive, negative]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORdyyfTFKQLL"
   },
   "source": [
    "### 1.2 Neural Framework\n",
    "\n",
    "We'll use [Keras](https://keras.io/), a high-level deep learning framework that is compatible with multiple backend libraries, such as [Tensorflow](https://www.tensorflow.org/) and [Theano](https://github.com/Theano/Theano).\n",
    "\n",
    "As the backend, we will consider Tensorflow as one of the most popular deep learning libraries.\n",
    "\n",
    "Feel free to experiment with other popular frameworks like [Pytorch](https://pytorch.org/). <br>\n",
    "$\\rightarrow$ Some flexibility is always considered a bonus since some models might be available for only one particular framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqNNz_uTKKjM"
   },
   "source": [
    "## 2. Data pipeline\n",
    "\n",
    "As outlined in the first tutorial, we need to define a proper pipeline to convert initial textual input into a numerical format that is compatible with our models.\n",
    "\n",
    "This pipeline is usually summarized as follows:\n",
    "\n",
    "*   Dataset loading\n",
    "*   Dataset pre-processing\n",
    "*   Dataset conversion\n",
    "*   Model definition\n",
    "*   Training\n",
    "*   Evaluation\n",
    "\n",
    "In this tutorial, we'll propose our solution by following above schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYWKAvEUnknW"
   },
   "source": [
    "### 2.1 Data Loading\n",
    "\n",
    "First of all, we need to load our dataset. \n",
    "\n",
    "Contrarily to Part I, we go beyond dataset limitations and consider the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Rqsemm2N-nZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Encoding dataset...\")\n",
    "df = encode_dataset(\"aclImdb\")\n",
    "print(\"Encoding completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhkUJwDLJVSW"
   },
   "source": [
    "### 2.2 Data Pre-processing\n",
    "\n",
    "For the task of sentiment analysis, we've already defined the I/O of the model. However, each input sentence is viewed as a sequence of tokens.\n",
    "\n",
    "We've already introduced a simple text cleaning implementation (see Section 1.4). Thus, we quickly re-apply the text cleaning to our textual data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DeYdM6znVt9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda txt: text_prepare(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVaVmfUBH7Pu"
   },
   "source": [
    "### 2.3 Data Splitting\n",
    "\n",
    "We distinguish between train, validation and test splits.\n",
    "\n",
    "In particular, we define a validation set that helps us evaluating our model during the training routine and avoiding overfitting.\n",
    "\n",
    "Since the dataset is balanced we apply a random split as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "162jj9rlIAbU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = df[df['split'] == 'train']\n",
    "test_data = df[df['split'] == 'test']\n",
    "\n",
    "x_train = train_data['text'].values\n",
    "y_train = train_data['sentiment'].values\n",
    "\n",
    "x_test = test_data['text'].values\n",
    "y_test = test_data['sentiment'].values\n",
    "\n",
    "# Random split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  train_size=0.80,\n",
    "                                                  test_size=0.20,\n",
    "                                                  random_state=42)\n",
    "\n",
    "print('Dataset splits statistics: ')\n",
    "print(f'Train data: {x_train.shape}')\n",
    "print(f'Validation data: {x_val.shape}')\n",
    "print(f'Test data: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sArsncbnU-d"
   },
   "source": [
    "For tokenization, we have already defined a vocabulary, but we prefer to introduce a more efficient tool that Keras offers: the [Keras Tokenizer]((https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwWIGz6Frp3i"
   },
   "source": [
    "For simplicity, we define a simple high-level wrapper that comes in handy for what we have to do.\n",
    "\n",
    "**Note**: this tokenizer also supports pre-trained embedding models. In this case, an embedding matrix is built to directly initialize the embedding layer of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CweXSsVUL85n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class KerasTokenizer(object):\n",
    "    \"\"\"\n",
    "    A simple high-level wrapper for the Keras tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, build_embedding_matrix=False, embedding_dimension=None,\n",
    "                 embedding_model_type=None, tokenizer_args=None):\n",
    "        if build_embedding_matrix:\n",
    "            assert embedding_model_type is not None\n",
    "            assert embedding_dimension is not None and type(embedding_dimension) == int\n",
    "        self.build_embedding_matrix = build_embedding_matrix\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.embedding_model_type = embedding_model_type\n",
    "        self.embedding_model = None\n",
    "        self.embedding_matrix = None\n",
    "        self.vocab = None\n",
    "        tokenizer_args = {} if tokenizer_args is None else tokenizer_args\n",
    "        assert isinstance(tokenizer_args, dict) or isinstance(tokenizer_args, collections.OrderedDict)\n",
    "        self.tokenizer_args = tokenizer_args\n",
    "\n",
    "    def build_vocab(self, data, **kwargs):\n",
    "        print('Fitting tokenizer...')\n",
    "        self.tokenizer = tf.keras.preprocessing.text.Tokenizer(**self.tokenizer_args)\n",
    "        self.tokenizer.fit_on_texts(data)\n",
    "        print('Fit completed!')\n",
    "        self.vocab = self.tokenizer.word_index\n",
    "        if self.build_embedding_matrix:\n",
    "            print('Loading embedding model! It may take a while...')\n",
    "            self.embedding_model = load_embedding_model(model_type=self.embedding_model_type,\n",
    "                                                        embedding_dimension=self.embedding_dimension)\n",
    "            \n",
    "            print('Checking OOV terms...')\n",
    "            self.oov_terms = check_OOV_terms(embedding_model=self.embedding_model,\n",
    "                                             word_listing=list(self.vocab.keys()))\n",
    "\n",
    "            print('Building the embedding matrix...')\n",
    "            self.embedding_matrix = build_embedding_matrix(embedding_model=self.embedding_model,\n",
    "                                                           word_to_idx=self.vocab,\n",
    "                                                           vocab_size=len(self.vocab) + 1,          \n",
    "                                                           embedding_dimension=self.embedding_dimension,\n",
    "                                                           oov_terms=self.oov_terms)\n",
    "            print('Done!')\n",
    "\n",
    "\n",
    "    def get_info(self):\n",
    "        return {\n",
    "            'build_embedding_matrix': self.build_embedding_matrix,\n",
    "            'embedding_dimension': self.embedding_dimension,\n",
    "            'embedding_model_type': self.embedding_model_type,\n",
    "            'embedding_matrix': self.embedding_matrix.shape if self.embedding_matrix is not None else self.embedding_matrix,\n",
    "            'embedding_model': self.embedding_model,\n",
    "            'vocab_size': len(self.vocab) + 1,\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return text\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        if type(tokens) == str:\n",
    "            return self.tokenizer.texts_to_sequences([tokens])[0]\n",
    "        else:\n",
    "            return self.tokenizer.texts_to_sequences(tokens)\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return self.tokenizer.sequences_to_texts(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRYMhkx2qdNR"
   },
   "source": [
    "### Note\n",
    "Normally, we would make use of pre-trained word embeddings for all our dataset. \n",
    "\n",
    "However, the pre-trained embedding matrix might be huge, occupying a lot of memory. \n",
    "\n",
    "A good approach consists in determining the amount of used tokens in order to reduce the size of the embedding matrix. <br>\n",
    "$\\rightarrow$ Usually, it is also fairly acceptable to just consider tokens that are in the train set for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsPu8VgaNS3H",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer_args = {\n",
    "    'oov_token': 'UNK',\n",
    "}\n",
    "embedding_dimension = 50\n",
    "tokenizer = KerasTokenizer(tokenizer_args=tokenizer_args,\n",
    "                           build_embedding_matrix=True,\n",
    "                           embedding_dimension=embedding_dimension,\n",
    "                           embedding_model_type=\"glove\")\n",
    "tokenizer.build_vocab(x_train)\n",
    "tokenizer_info = tokenizer.get_info()\n",
    "print(f'Tokenizer info: {tokenizer_info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2PptUJnKOzE"
   },
   "source": [
    "### 2.4 Data Conversion\n",
    "\n",
    "We are ready to convert input data into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw_t8aJ1rp3i"
   },
   "source": [
    "#### Labels\n",
    "\n",
    "Note that, we have also to convert labels into numerical format, since the neural network baselines will be trained to minimize an objective function that is based on input labels. However, in this example, labels are already in numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H7gZdbdrp3i"
   },
   "source": [
    "#### Padding\n",
    "\n",
    "Since neural networks are trained with data mini-batches, we need to make sure that each of our inputs are of the same size in order to store them into **rectangular matrices**. This process is usually referred to as **padding** and its applicability really depends on how you define mini-batches.\n",
    "\n",
    "For instance, you might want to pad each mini-batch **individually**, pad **all of them** to the same size or **not apply pad at all**! In the latter case, it is mandatory to define mini-batches with samples of the same length. This process is usually a bit slower but saves a lot of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_61Gd-tNrp3i"
   },
   "source": [
    "Padding has also the **drawback of potential data truncation**. Usually, we can determine the padding size in order to handle the majority of our data (95-99%).\n",
    "The are always outliers that might over-estimate the padding size and lead to high memory usage!\n",
    "\n",
    "For the purpose of this tutorial, we apply a simple global padding based on the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19FxDv4nq20n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_text(texts, tokenizer, is_training=False, max_seq_length=None):\n",
    "    \"\"\"\n",
    "    Converts input text sequences using a given tokenizer\n",
    "\n",
    "    :param texts: either a list or numpy ndarray of strings\n",
    "    :tokenizer: an instantiated tokenizer\n",
    "    :is_training: whether input texts are from the training split or not\n",
    "    :max_seq_length: the max token sequence previously computed with\n",
    "    training texts.\n",
    "\n",
    "    :return\n",
    "        text_ids: a nested list on token indices\n",
    "        max_seq_length: the max token sequence previously computed with\n",
    "        training texts.\n",
    "    \"\"\"\n",
    "    text_ids = tokenizer.convert_tokens_to_ids(texts)\n",
    "\n",
    "    # Padding\n",
    "    if is_training:\n",
    "        max_seq_length = int(np.quantile([len(seq) for seq in text_ids], 0.99))\n",
    "    else:\n",
    "        assert max_seq_length is not None\n",
    "\n",
    "    text_ids = [seq + [0] * (max_seq_length - len(seq)) for seq in text_ids]\n",
    "    text_ids = np.array([seq[:max_seq_length] for seq in text_ids])\n",
    "\n",
    "    if is_training:\n",
    "        return text_ids, max_seq_length\n",
    "    else:\n",
    "        return text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH-o7zzyrp3i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "x_train, max_seq_length = convert_text(texts=x_train, tokenizer=tokenizer, is_training=True)\n",
    "print(f\"Max token sequence: {max_seq_length}\")\n",
    "print(f'X train shape: {x_train.shape}')\n",
    "print(f'Y train shape: {y_train.shape}')\n",
    "\n",
    "# Val\n",
    "x_val = convert_text(texts=x_val, tokenizer=tokenizer, is_training=False, max_seq_length=max_seq_length)\n",
    "\n",
    "print(f'X val shape: {x_val.shape}')\n",
    "print(f'Y val shape: {y_val.shape}')\n",
    "\n",
    "# Test\n",
    "x_test = convert_text(texts=x_test, tokenizer=tokenizer, is_training=False, max_seq_length=max_seq_length)\n",
    "\n",
    "print(f'X test shape: {x_test.shape}', x_test.shape)\n",
    "print(f'Y test shape: {y_test.shape}', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dS1nWn0NKQR5"
   },
   "source": [
    "### 2.5 Model Definition\n",
    "\n",
    "Keras offers a simple and high-level interface for model definition. <br>\n",
    "$\\rightarrow$ This allows you to define simple models in very few line of codes.\n",
    "\n",
    "There are several ways of defining a Keras model. \n",
    "\n",
    "For the purpose of this tutorial, we'll define our model using the [Keras Sequential APIs](https://keras.io/guides/sequential_model/).\n",
    "\n",
    "Our model will be a simple MLP. We won't explore advanced architectures like RNNs and CNNs. However, feel free to try them out as exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PIUklixvz9s",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def create_model(layers_info: List[Dict], compile_info: Dict) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a Keras model given a list of layer information\n",
    "\n",
    "    :param layers_info: a list of dictionaries, one for each layer\n",
    "    :param compile_info: dictionary containing compile information\n",
    "\n",
    "    :return\n",
    "        model: the built keras sequential model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    for info_idx, info in enumerate(layers_info):\n",
    "        # Make sure the last layer has softmax activation and has 2 units for correct learning.\n",
    "        if info_idx == len(layers_info) - 1:\n",
    "            assert info['activation'] == 'softmax'\n",
    "            assert info['units'] == 2\n",
    "\n",
    "        layer = info['layer'](**{key: value for key, value in info.items() if key != 'layer'})\n",
    "        model.add(layer)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(**compile_info)\n",
    "    return model\n",
    "\n",
    "\n",
    "class AvgSentenceEmbedding(layers.Layer):\n",
    "\n",
    "    def call(self, inputs, mask=None, training=False, **kwargs):\n",
    "        float_mask = tf.cast(mask, inputs.dtype)\n",
    "        masked_inputs = inputs * float_mask[:, :, None]\n",
    "        return tf.reduce_sum(masked_inputs, axis=1) / tf.reduce_sum(float_mask, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4AyNbiFrp3i",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers_info = [\n",
    "    {\n",
    "        \"layer\": layers.Embedding,\n",
    "        \"output_dim\": 50,\n",
    "        \"input_dim\": tokenizer_info['vocab_size'],\n",
    "        \"input_length\": max_seq_length,\n",
    "        \"weights\": tokenizer.embedding_matrix if tokenizer.embedding_matrix is None else [tokenizer.embedding_matrix],\n",
    "        \"mask_zero\": True,\n",
    "        \"name\": \"embedding_layer\"\n",
    "    },\n",
    "    {\n",
    "        \"layer\": AvgSentenceEmbedding,\n",
    "        \"name\": \"sentence_embedding\"\n",
    "    },\n",
    "    {\n",
    "        'layer': layers.Dense,\n",
    "        \"units\": 256,\n",
    "        \"activation\": \"relu\",\n",
    "        \"name\": \"dense_1\"\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 64,\n",
    "        \"activation\": \"relu\",\n",
    "        \"name\": \"dense_2\"\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 2,\n",
    "        \"activation\": \"softmax\",\n",
    "        \"name\": \"logits\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8FBWiLVrp3j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compile_info = {\n",
    "    'optimizer': keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    'loss': 'sparse_categorical_crossentropy',\n",
    "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
    "}\n",
    "model = create_model(layers_info, compile_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsSz83VK5w57"
   },
   "source": [
    "#### Embedding Layer\n",
    "\n",
    "The embedding layer is the backbone of any NLP neural network architecture. It essentially associates a token index to its corresponding trainable embedding vector.\n",
    "\n",
    "In the above example, we are defining:\n",
    "\n",
    "* `output_dim`: the dimension of the embedding vector\n",
    "* `input_dim`: the size of the vocabulary\n",
    "* `input_length`: the expected input tokens sequence length\n",
    "* `weights`: we can initialize the embedding matrix (token indices -> embeddings) with our pre-trained embedding model\n",
    "* `mask_zero`: Keras automatically ignores `PAD` tokens (token index equal to $0$). This mask is propagated to subsequent layers up to the objective function. This is particularly important for sequence tagging. If enabled with RNNs, make sure you apply a POST padding (as in our example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJVARaz863mV"
   },
   "source": [
    "#### Sentence Embedding\n",
    "\n",
    "Intuitively, we need to classify the entire sentence as either positive or negative. However, our input is a sequence of tokens.\n",
    "\n",
    "One very simple way consists in obtaining the sentence embedding is the mean of its constituting token embeddings. There are other several ways to achieve the same result with much higher quality!\n",
    "\n",
    "In this example, we make use of the Keras Lambda layer to define input sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFH72NT9KVBx"
   },
   "source": [
    "### 2.6 Training and Inference Setup\n",
    "\n",
    "At this point, we can start training and evaluating our model.\n",
    "\n",
    "Just like before, Keras offers simple high-level APIs for this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHyO-DUz0mqq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from functools import partial\n",
    "import random\n",
    "\n",
    "def show_history(history: keras.callbacks.History):\n",
    "    \"\"\"\n",
    "    Shows training history data stored by the History Keras callback\n",
    "\n",
    "    :param history: History Keras callback\n",
    "    \"\"\"\n",
    "    history_data = history.history\n",
    "    print(f\"Displaying the following history keys: {history_data.keys()}\")\n",
    "\n",
    "    for key, value in history_data.items():\n",
    "        if not key.startswith('val'):\n",
    "            fig, ax = plt.subplots(1, 1)\n",
    "            ax.set_title(key)\n",
    "            ax.plot(value)\n",
    "            if f'val_{key}' in history_data:\n",
    "                ax.plot(history_data[f'val_{key}'])\n",
    "            else:\n",
    "                print(f\"Couldn't find validation values for metric: {key}\")\n",
    "\n",
    "            ax.set_ylabel(key)\n",
    "            ax.set_xlabel('epoch')\n",
    "            ax.legend(['train', 'val'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2JeGddFrp3j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model: keras.Model,\n",
    "                x_train: np.ndarray,\n",
    "                y_train: np.ndarray,\n",
    "                x_val: np.ndarray,\n",
    "                y_val: np.ndarray,\n",
    "                training_info: Dict,\n",
    "                show=True):\n",
    "    \"\"\"\n",
    "    Training routine for the Keras model.\n",
    "    At the end of the training, retrieved History data is shown.\n",
    "\n",
    "    :param model: Keras built model\n",
    "    :param x_train: training data in np.ndarray format\n",
    "    :param y_train: training labels in np.ndarray format\n",
    "    :param x_val: validation data in np.ndarray format\n",
    "    :param y_val: validation labels in np.ndarray format\n",
    "    :param training_info: dictionary storing model fit() argument information\n",
    "\n",
    "    :return\n",
    "        model: trained Keras model\n",
    "    \"\"\"\n",
    "    print(f\"Start training! \\nParameters: {training_info}\")\n",
    "    history = model.fit(x=x_train, y=y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        **training_info)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    if show:\n",
    "        print(\"Showing history...\")\n",
    "        show_history(history)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLQt4IYdrp3j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_data(model: keras.Model,\n",
    "                 x: np.ndarray,\n",
    "                 prediction_info: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inference routine of a given input set of examples\n",
    "\n",
    "    :param model: Keras built and possibly trained model\n",
    "    :param x: input set of examples in np.ndarray format\n",
    "    :param prediction_info: dictionary storing model predict() argument information\n",
    "\n",
    "    :return\n",
    "        predictions: predicted labels in np.ndarray format\n",
    "    \"\"\"\n",
    "    print(f'Starting prediction: \\n{prediction_info}')\n",
    "    print(f'Predicting on {x.shape[0]} samples')\n",
    "    predictions = model.predict(x, **prediction_info)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQeD2KU0rp3j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions: np.ndarray,\n",
    "                         y: np.ndarray,\n",
    "                         metrics: List[Callable],\n",
    "                         metric_names: List[str]):\n",
    "    \"\"\"\n",
    "    Evaluates given model predictions on a list of metric functions\n",
    "\n",
    "    :param predictions: model predictions in np.ndarray format\n",
    "    :param y: ground-truth labels in np.ndarray format\n",
    "    :param metrics: list of metric functions\n",
    "    :param metric_names: list of metric names\n",
    "\n",
    "    :return\n",
    "        metric_info: dictionary containing metric values for each input metric\n",
    "    \"\"\"\n",
    "    assert len(metrics) == len(metric_names)\n",
    "    print(f\"Evaluating predictions! Total samples: {y.shape[0]}\")\n",
    "\n",
    "    metric_info = {}\n",
    "    for metric, metric_name in zip(metrics, metric_names):\n",
    "        metric_value = metric(y_pred=predictions, y_true=y)\n",
    "        metric_info[metric_name] = metric_value\n",
    "\n",
    "    return metric_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiQ44_U1rp3j",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "training_info = {\n",
    "    'verbose': 1,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "}\n",
    "model = train_model(model=model, x_train=x_train, y_train=y_train,\n",
    "                    x_val=x_val, y_val=y_val, training_info=training_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuXDjgGFrp3j"
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "prediction_info = {\n",
    "    'batch_size': 64,\n",
    "    'verbose': 1\n",
    "}\n",
    "test_predictions = predict_data(model=model, x=x_test,\n",
    "                                      prediction_info=prediction_info)\n",
    "test_predictions = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "# Evaluation\n",
    "metrics = [\n",
    "    accuracy_score,\n",
    "    partial(f1_score, pos_label=1, average='binary')\n",
    "]\n",
    "metric_names = [\n",
    "    \"accuracy\",\n",
    "    \"binary_f1\"\n",
    "]\n",
    "metric_info = evaluate_predictions(predictions=test_predictions,\n",
    "                                   y=y_test,\n",
    "                                   metrics=metrics,\n",
    "                                   metric_names=metric_names)\n",
    "print(f'Metrics info: \\n{metric_info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_Wr4_jirp3j"
   },
   "source": [
    "# PART IV\n",
    "\n",
    "*   **Regularization** techniques: L2, Dropout, Early Stopping.\n",
    "*   **Good practices** for experimenting.\n",
    "*   Intro to **transformers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u36ww6HxKZso"
   },
   "source": [
    "## 1. Regularization Techniques\n",
    "\n",
    "A curious reader might be wondering how to correctly train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDvYcP0jrp3j"
   },
   "source": [
    "#### Learning Curves\n",
    "\n",
    "One of the best indicators is the learning curve plot, where loss information of train and val splits is shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wdOk5Rjrp3j"
   },
   "source": [
    "#### Overfitting\n",
    "\n",
    "If the model quickly overfits, we might need to apply some significant regularization technique. \n",
    "\n",
    "Overfit can happen for a lot of different reasons, but it is usually mainly attributed to some biases in the data or to the high representational capacity of the model. In the latter case, we might want to \"simplify\" our architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfxOEWkXrp3k"
   },
   "source": [
    "#### Underfitting\n",
    "\n",
    "On the other hand, if the model strongly underfits, we should increase its capacity. This is usually done by increasing the model depth, i.e. by adding some additional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3aYh8a7rp3k"
   },
   "source": [
    "In this section, we'll briefly outline some well-known regularization techniques and show you how to implement them. Remember that model regularization is a technique to reduce the generalization error without reducing the optimization error.\n",
    "\n",
    "We leave further experiments as exercise. Feel free to try out different combinations of regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKsfePwOKfM-"
   },
   "source": [
    "### 1.1 Early Stopping\n",
    "\n",
    "Early stopping is a widely applied regularization technique to avoid model overfitting on the train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjxXIQuWrp3k"
   },
   "source": [
    "#### \"How many epochs/steps should I train my model?\"\n",
    "\n",
    "A good answer can be informally defined as \"Until your model starts performing worse on the validation set\".\n",
    "\n",
    "That's basically what Early Stopping does. Intuitively, we stop the learning process after the model has stopped improving on a certain validation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJhxpa7nrp3k"
   },
   "source": [
    "#### Which validation metric?\n",
    "\n",
    "Which one? The best practice goes for the validation loss, however, in challenging scenarios with unbalanced data, you might want to achieve the best possible model performance. More generally, you might want to track multiple metrics at the same time!\n",
    "\n",
    "Here is some useful reading material regarding Early Stopping:\n",
    "\n",
    "* [Deep learning book, section 7.8](https://www.deeplearningbook.org/contents/regularization.html)\n",
    "* [Multiple metrics Early Stopping](http://alexadam.ca/ml/2018/08/03/early-stopping.html)\n",
    "* [Early Stopping blog](https://towardsdatascience.com/the-million-dollar-question-when-to-stop-training-deep-learning-models-fa9b488ac04d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSTBN8um9a1C",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_info = {\n",
    "    'verbose': 1,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "    'callbacks': [keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                patience=5,\n",
    "                                                restore_best_weights=True)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iuOM6hh9nq7"
   },
   "source": [
    "#### Early Stopping in Keras\n",
    "\n",
    "Keras defines early stopping via a callback.\n",
    "\n",
    "The `monitor` argument tells the callback on which metric to enforce early stopping.\n",
    "\n",
    "The `patience` arguments defines the number of epochs to wait for metric improvement. If more than `patience` epochs occur since last improvement, the training routine is halted.\n",
    "\n",
    "The `restore_best_weights` argument tells the callback to reload model weights associated to the best performing epoch (w.r.t. monitored metric).\n",
    "\n",
    "For more details, please check the official [documentation](https://keras.io/api/callbacks/early_stopping/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM3NdtfMKifO"
   },
   "source": [
    "### 1.2 L2 or Weight Decay\n",
    "\n",
    "It is also known as Ridge regression or Tikhonov regularization.\n",
    "\n",
    "It is the simplest norm penalty regularization:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{J}(x, y; \\theta) = J(x, y; \\theta) + \\alpha \\Omega(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "where $\\Omega(\\theta) = \\frac{1}{2}||\\theta||_2^2$\n",
    "\n",
    "In simple terms, weight decays rescales weights along eigenvectors directions (of the Hessian matrix, $H$). <br>\n",
    "Specifically, each eigenvector of $H$ is rescaled by a factor:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\lambda_i}{\\lambda_i + \\alpha}\n",
    "\\end{align}\n",
    "\n",
    "Along directions where eigenvalues are large, weight decay has little or no effect. On the other hand, components with small eigenvalues will shrink to have nearly zero magnitude. <br>\n",
    "$\\rightarrow$ The objective function on those direction won't change too much (minor curvature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZjOWEFHrp3k"
   },
   "source": [
    "In Keras, weight decay has to be defined for each layers as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jo7-YW63BKIH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers_info = [\n",
    "    # {...}\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 128,\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_regularizer\": keras.regularizers.l2(0.01)\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 16,\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_regularizer\": keras.regularizers.l2(0.01)\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 2,\n",
    "        \"activation\": None\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq3xXrs6BaG5"
   },
   "source": [
    "In this example, the first two Dense layers will have a weight decay with coefficient $\\alpha = 0.01$.\n",
    "\n",
    "A good practice, consists in defining a single $\\alpha$ for all layers, but it not mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70q13QkSKjm8"
   },
   "source": [
    "### 1.3 Dropout\n",
    "\n",
    "Dropout originates as a smart solution to overcome the drawback of **bagging**. The idea is to consider sub-networks of the original architectures as an ensemble.\n",
    "\n",
    "These sub-networks are created by literally 'dropping out' some neurons in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--kEhC3Frp3k"
   },
   "source": [
    "#### Advantage\n",
    "\n",
    "The additional advantage w.r.t. to bagging is that these sub-networks share their weights. Thus, it is possible to train an exponential number of sub-networks with a tractable amount of memory.\n",
    "\n",
    "However, the key insight is that dropout forces each hidden unit to perform well regardless of other units in the sub-network. Thus, each unit is regularized to be a good feature in many contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgjq3cqDrp3k"
   },
   "source": [
    "In Keras, dropout takes the form of a layer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htN2TlhotD-n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers_info = [\n",
    "    # {...}\n",
    "    {\n",
    "        \"layer\": layers.Dense,\n",
    "        \"units\": 128,\n",
    "        \"activation\": \"relu\",\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dropout,\n",
    "        \"rate\": 0.2\n",
    "    },\n",
    "    {\n",
    "        \"units\": 16,\n",
    "        \"activation\": \"relu\",\n",
    "        \"kernel_regularizer\": keras.regularizers.l2(0.01)\n",
    "    },\n",
    "    {\n",
    "        \"layer\": layers.Dropout,\n",
    "        \"rate\": 0.2\n",
    "    },\n",
    "    {\n",
    "        \"units\": 2,\n",
    "        \"activation\": None\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iJXcFmluHqX"
   },
   "source": [
    "In this example, we are adding two additional Dropout layers to our model, both of them with dropout rate set to $0.2$. This means, that there's a $20\\%$ chance of dropping input units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fes2Uu03rp3k"
   },
   "source": [
    "#### Do It Yourself\n",
    "\n",
    "Have fun experimenting with different combinations of regularization techniques and inspect the achieved learning curves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZcU5LS6rp3l"
   },
   "source": [
    "## 2. Good Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1efSCTI8rp3l"
   },
   "source": [
    "#### Robust Evaluation Routine\n",
    "\n",
    "Depending on the given setting, you might need to set up a specific evaluation routine:\n",
    "* Train and Test\n",
    "* K-fold Cross-Validation\n",
    "* Leave-one-out\n",
    "\n",
    "Model evaluation should take into account:\n",
    "* Corpus size $\\rightarrow$ number of samples\n",
    "* Model variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq06DvoGrp3l"
   },
   "source": [
    "#### Reproducibility\n",
    "\n",
    "Reproducibility is a fundamental evaluation property in order to make **fair evaluation**.\n",
    "*  Clear and reproducible data pipeline\n",
    "*  Model training and evaluation with **multiple** and **fixed** seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kncz1R1rp3l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_reproducibility(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr4X3onLrp3l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seeds = [42, 2022, 1500, 1337, 666]\n",
    "avg_metric_info = {}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'Running with seed: {seed}')\n",
    "    set_reproducibility(seed)\n",
    "    \n",
    "    model = create_model(layers_info, compile_info) \n",
    "    model = train_model(model=model, x_train=x_train, y_train=y_train,\n",
    "                        x_val=x_val, y_val=y_val, training_info=training_info,\n",
    "                        show=False)\n",
    "\n",
    "    test_predictions = predict_data(model=model, x=x_test,\n",
    "                                          prediction_info=prediction_info)\n",
    "    test_predictions = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "    metric_info = evaluate_predictions(predictions=test_predictions,\n",
    "                                       y=y_test,\n",
    "                                       metrics=metrics,\n",
    "                                       metric_names=metric_names)\n",
    "    for key, value in metric_info.items():\n",
    "        avg_metric_info.setdefault(key, []).append(value)\n",
    "\n",
    "avg_metric_info = {key: np.mean(value) for key, value in avg_metric_info.items()}\n",
    "print(f'Metrics info: \\n{avg_metric_info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkD7ZpXvrp3l"
   },
   "source": [
    "#### Reproducibility with different backends\n",
    "\n",
    "I recommend the [framework determinism](https://github.com/NVIDIA/framework-determinism) framework to set up a reproducible enviroment with popular backends like\n",
    "* Tensorflow\n",
    "* Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rsNGgjYrp3l"
   },
   "source": [
    "## 3. Transformers\n",
    "\n",
    "Defining simple baseline is usually a good starting point to:\n",
    "* Task benchmarking\n",
    "* Code testing\n",
    "\n",
    "However, we know that Transformer-based models are they baseline to go when assessing the challenge of a new task or benchmarking on a novel corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAcxxSNGrp3l"
   },
   "source": [
    "#### Transformers in Keras\n",
    "\n",
    "We'll rely on well known libraries for defining a simple Transformer-based baseline:\n",
    "* HuggingFace $\\rightarrow$ transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwPJDi6C4oUQ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmP7RfMRrp3l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig, PretrainedConfig\n",
    "\n",
    "def create_bert_model(compile_info: List[Dict],\n",
    "                      pretrained_model_name_or_path: str) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Create a Keras model given a list of layer information\n",
    "\n",
    "    :param layers_info: a list of dictionaries, one for each layer\n",
    "    :param compile_info: dictionary containing compile information\n",
    "\n",
    "    :return\n",
    "        model: the built keras sequential model\n",
    "    \"\"\"\n",
    "    # Load pre-trained model\n",
    "    bert = TFDistilBertForSequenceClassification.from_pretrained(pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "                                                                 num_labels=2)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(**compile_info)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnqQ40xTrp3l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = 'distilbert-base-uncased'\n",
    "compile_info = {\n",
    "    'optimizer': keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    'loss': 'sparse_categorical_crossentropy',\n",
    "    'metrics': [keras.metrics.SparseCategoricalAccuracy()],\n",
    "}\n",
    "# Training\n",
    "training_info = {\n",
    "    'verbose': 1,\n",
    "    'epochs': 2,\n",
    "    'batch_size': 16,\n",
    "}\n",
    "# Inference\n",
    "prediction_info = {\n",
    "    'batch_size': 16,\n",
    "    'verbose': 1\n",
    "}\n",
    "model = create_bert_model(compile_info=compile_info,\n",
    "                          pretrained_model_name_or_path=pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkOM4Qy8rp3l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seeds = [42, 2022, 1500, 1337, 666]\n",
    "avg_metric_info = {}\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'Running with seed: {seed}')\n",
    "    set_reproducibility(seed)\n",
    "    \n",
    "    model = create_bert_model(compile_info=compile_info,\n",
    "                          pretrained_model_name_or_path=pretrained_model_name_or_path)\n",
    "    model = train_model(model=model, x_train=x_train, y_train=y_train,\n",
    "                        x_val=x_val, y_val=y_val, training_info=training_info,\n",
    "                        show=False)\n",
    "\n",
    "    test_predictions = predict_data(model=model, x=x_test,\n",
    "                                          prediction_info=prediction_info)\n",
    "    test_predictions = np.argmax(test_predictions, axis=-1)\n",
    "\n",
    "    metric_info = evaluate_predictions(predictions=test_predictions,\n",
    "                                       y=y_test,\n",
    "                                       metrics=metrics,\n",
    "                                       metric_names=metric_names)\n",
    "    for key, value in metric_info.items():\n",
    "        avg_metric_info.setdefault(key, []).append(value)\n",
    "\n",
    "avg_metric_info = {key: np.mean(value) for key, value in avg_metric_info.items()}\n",
    "print(f'Metrics info: \\n{avg_metric_info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j3srU3qX7Be"
   },
   "source": [
    "# Takeaway Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXd-EA6orp3l"
   },
   "source": [
    "#### Teaching \n",
    "\n",
    "Your TA should be quite thirsty and tired at this point, make sure they are okay even though you didn't understand a single word of their \"English\". Possibly, offer them a beer ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRge6E5Drp3m"
   },
   "source": [
    "#### Dense vs Sparse Embeddings\n",
    "\n",
    "Dense word embeddings are very cool w.r.t. to sparse ones. However, we've just explored fixed embedding models: each word has its own embedding vector independently of the current context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EyH7Hwprp3m"
   },
   "source": [
    "#### Out-of-vocabulary Terms\n",
    "OOV terms are everywhere, when we should worry about them? Have a look at pre-trained embedding models if they are suitable to your domain to drastically reduce this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekJRVODarp3m"
   },
   "source": [
    "#### Data Pipeline\n",
    "\n",
    "Think about the whole data pipeline! Investigate your data before any experiment and start with simple baselines. You have to understand the setting before anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJJeSx7Hrp3m"
   },
   "source": [
    "#### Regularization\n",
    "Depending on the given scenario and related experimental results, you might want to consider regularization techniques to achieve better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ilhj-bbirp3m"
   },
   "source": [
    "#### Robust and Reproducible Experiments\n",
    "\n",
    "Define consistent and reproducible experiments for fair model comparison $\\rightarrow$ control training and evaluation stochasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-VuPx8Drp3m"
   },
   "source": [
    "#### Transformers\n",
    "\n",
    "Transformers models are everywhere and are the way-to-go baseline. There exist several libraries for quick experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbzMCMfprp3m"
   },
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "-5qeqzylrp3b",
    "GoSJR73EF2Tf",
    "F7RNPP1N9FxD",
    "tbuS9RBprp3d",
    "1DGpClSWrp3f",
    "3Z3z2u6orp3g",
    "3MpHvK4FFeap",
    "8skS3zDirp3h",
    "ORdyyfTFKQLL",
    "uw_t8aJ1rp3i",
    "vsSz83VK5w57",
    "qJVARaz863mV",
    "gDvYcP0jrp3j",
    "5wdOk5Rjrp3j",
    "VjxXIQuWrp3k",
    "5iuOM6hh9nq7",
    "fes2Uu03rp3k",
    "1efSCTI8rp3l",
    "kkD7ZpXvrp3l",
    "ZXd-EA6orp3l",
    "lRge6E5Drp3m",
    "2EyH7Hwprp3m",
    "ekJRVODarp3m",
    "sJJeSx7Hrp3m",
    "Ilhj-bbirp3m",
    "b-VuPx8Drp3m"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
