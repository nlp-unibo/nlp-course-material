{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248026a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial 3\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Transformers, Huggingface, Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f656c",
   "metadata": {
    "id": "m3wzWLL-LiKd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "* Eleonora Mancini -> e.mancini@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe496b",
   "metadata": {
    "id": "gl48Am5trp3Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART I ($\\sim$30 mins)\n",
    "\n",
    "*   Text encoding with transformers.\n",
    "*   Model definition.\n",
    "*   Model training and evaluation with huggingface APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3fbeb",
   "metadata": {
    "id": "D4anSmM4rp3Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PART II ($\\sim$30 mins)\n",
    "\n",
    "*   Prompting 101\n",
    "*   Sentiment analysis with prompting\n",
    "*   Advanced prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1151d4",
   "metadata": {
    "id": "c4-E45fvrp3Z",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Preliminaries\n",
    "\n",
    "First of all, we need to import some useful packages that we will use during this hands-on session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66be93a",
   "metadata": {
    "id": "rUXZLYya69wc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# system packages\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "import sys\n",
    "\n",
    "# data and numerical management packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# useful during debugging (progress bars)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98e9d293",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->transformers) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: requests in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: evaluate in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bitsandbytes in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (0.48.2)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from bitsandbytes) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/fruggeri/virtualenvs/rise-venv/lib/python3.11/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install evaluate\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51c7b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9954d2c8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 10 14:41:31 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    Off |   00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   48C    P8              3W /   50W |      18MiB /   8188MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A            3133      G   /usr/lib/xorg/Xorg                        4MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932eb9e",
   "metadata": {
    "id": "qImKj2Mu7LCX",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "We will use the IMDB dataset first introduced in tutorial 1.\n",
    "\n",
    "* [**Stats**] A dataset of 50k sentences used for sentiment analysis: 25k with positive sentiment, 25k with negative one.\n",
    "* [**Sentiment**] We consider sentiment labels for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fdf63f",
   "metadata": {
    "id": "NSvqBcKJ7iTY",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with tarfile.open(download_path) as loaded_tar:\n",
    "        loaded_tar.extractall(extract_path)\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11619d9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: /home/fruggeri/Downloads/Tutorial 3\n"
     ]
    }
   ],
   "source": [
    "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "dataset_name = \"aclImdb\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_tar_path = dataset_folder.joinpath(\"Movies.tar.gz\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_tar_path.exists():\n",
    "    download_dataset(dataset_tar_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_tar_path, dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b5072",
   "metadata": {
    "id": "pv3NW1SNrp3a",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Data Format\n",
    "\n",
    "Just like in the first assignment, we need a **high level view** of the dataset that is helpful to our needs. \n",
    "\n",
    "We encode the dataset into a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db381745",
   "metadata": {
    "id": "P05YfYCe7qCj",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_rows = []\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        folder = dataset_folder.joinpath(dataset_name, split, sentiment)\n",
    "        for file_path in folder.glob('*.txt'):            \n",
    "            with file_path.open(mode='r', encoding='utf-8') as text_file:\n",
    "                text = text_file.read()\n",
    "                score = file_path.stem.split(\"_\")[1]\n",
    "                score = int(score)\n",
    "                file_id = file_path.stem.split(\"_\")[0]\n",
    "\n",
    "                num_sentiment = 1 if sentiment == 'pos' else 0\n",
    "\n",
    "                dataframe_row = {\n",
    "                    \"file_id\": file_id,\n",
    "                    \"score\": score,\n",
    "                    \"sentiment\": num_sentiment,\n",
    "                    \"split\": split,\n",
    "                    \"text\": text\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "584277b8",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "folder = Path.cwd().joinpath(\"Datasets\", \"Dataframes\", dataset_name)\n",
    "if not folder.exists():\n",
    "    folder.mkdir(parents=True)\n",
    "\n",
    "# transform the list of rows in a proper dataframe\n",
    "df = pd.DataFrame(dataframe_rows)\n",
    "df = df[[\"file_id\", \n",
    "         \"score\",\n",
    "         \"sentiment\",\n",
    "         \"split\",\n",
    "         \"text\"]\n",
    "       ]\n",
    "df_path = folder.with_name(dataset_name + \".pkl\")\n",
    "df.to_pickle(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecf8a6",
   "metadata": {
    "id": "Bjf3k-qVrp3b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART I\n",
    "\n",
    "*   Text encoding with Transformers.\n",
    "*   Model definition.\n",
    "*   Model training and evaluation with huggingface APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c6a6d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Text encoding with Transformers.\n",
    "\n",
    "In Tutorial 1, we have seen how to define standard machine learning models to address sentiment classification.\n",
    "\n",
    "However, we know that Transformer-based models are one of the strongest baselines when assessing a task or benchmarking on a novel corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bac417",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before defining our transformer-based classifier, we need to encode text inputs into numerical format.\n",
    "\n",
    "As in Tutorial 1, we are going to **tokenize** input texts to perform token indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375eb13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Encoding the dataset\n",
    "\n",
    "First, we are going to use ``datasets`` library to encode our dataset into a handy wrapper for computational speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b808d5bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Slicing for showcasing purposes only!\n",
    "train_df = df.loc[df['split'] == \"train\"].sample(frac=1.0)[:5000]\n",
    "test_df = df.loc[df['split'] == \"test\"]\n",
    "\n",
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87decf3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the newly defined `Dataset` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "881491ae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250c1326",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Tokenization\n",
    "\n",
    "Transformers typically use [SentencePiece tokenizer](https://github.com/google/sentencepiece) to perform sub-word level tokenization.\n",
    "\n",
    "In particular, the `transformers` library offers the `AutoTokenizer` class to quickly retrieve our chosen transformer's ad-hoc tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b30591",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_card = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287a7224",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `model_card` variable defines the *path* where to look for our pre-trained model.\n",
    "\n",
    "You can check [huggingface's hub](https://huggingface.co/models) model hub to pick the model card according to your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb5e33",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We proceed on tokenizing movie reviews text with our tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bc272c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140ab7f2884541f3a1bf2d6cbe24a1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b344787d94946239149855808a259f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_text(texts):\n",
    "    return tokenizer(texts['text'], truncation=True)\n",
    "\n",
    "train_data = train_data.map(preprocess_text, batched=True)\n",
    "test_data = test_data.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb9a10",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the preprocess `Dataset` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a668aae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['file_id', 'score', 'sentiment', 'split', 'text', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 25000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce543485",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 5589, 1999, 1037, 12127, 1005, 1055, 17899, 1010, 1998, 2023, 2143, 2001, 3491, 2044, 2057, 2018, 2525, 2464, 1037, 2025, 2061, 2307, 2143, 1998, 4194, 1999, 1037, 2871, 3371, 6594, 1012, 2130, 2295, 2009, 2001, 11757, 2397, 1998, 2057, 2020, 16040, 1010, 1996, 2972, 4378, 2428, 5632, 2009, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 7714, 1045, 2245, 1996, 2143, 2001, 26316, 1999, 2035, 1996, 2157, 7516, 1010, 1998, 1045, 3866, 1996, 21864, 15952, 3459, 1997, 3494, 1012, 2027, 2428, 4982, 2006, 2017, 1999, 1996, 2143, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['input_ids'][12][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31f6246b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_data['attention_mask'][12][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a13d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can perform some quick *sanity check* to evaluate the tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f216e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I participate in a Filmmaker's Symposium, and this film was shown after we had already seen a not so great film and participated in a 40 minute discussion. Even though it was incredibly late and we we\n",
      "\n",
      "\n",
      "[CLS] i participate in a filmmaker ' s symposium, and this film was shown after we had already seen a not so great film and participated in a 40 minute discussion. even though it was incredibly late a\n"
     ]
    }
   ],
   "source": [
    "original_text = train_data['text'][12]\n",
    "decoded_text = tokenizer.decode(train_data['input_ids'][12])\n",
    "\n",
    "print(original_text[:200])\n",
    "print()\n",
    "print()\n",
    "print(decoded_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b0876",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 Vocabulary\n",
    "\n",
    "We **do not** necessarily need to build a vocabulary since transformers already come with their own! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0050d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**However**, it is still possible to add new tokens to the vocabulary to adapt the model to the given use case.\n",
    "\n",
    "```\n",
    "tokenizer.add_tokens(new_tokens=new_tokens)\n",
    "```\n",
    "\n",
    "The transformer vocabulary will update its **unusued** vocabulary indexes with newly provided tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ad541",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.4 Special tokens\n",
    "\n",
    "**Pay attention** to used special tokens and their corresponding token ids.\n",
    "\n",
    "Each transformer models has its own special tokens ([CLS], [SEP], [PAD], [EOS], etc...).\n",
    "\n",
    "Thus, the same special token may be mapped to different token ids in distinct transformer models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db7d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.5 Text cleaning\n",
    "\n",
    "We didn't perform any kind of text cleaning before performing text encoding.\n",
    "\n",
    "This is usually because transformer tokenizers **have their own text cleaning process** to perform tokenization.\n",
    "\n",
    "Thus, models **may be sensitive** to custom operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a0a569f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['couldn', \"'\", 't']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"couldn't\"\n",
    "encoded_example = tokenizer.encode_plus(example_text,\n",
    "                                        add_special_tokens=False)\n",
    "print(encoded_example.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "793cb07a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'one', 'point', ',', 'some', 'kids', 'are', 'wandering', 'through', 'the', 'deeper', 'levels', ',', 'exploring', '.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"At one point,some kids are wandering\n",
    "through the deeper levels, exploring.\"\"\"\n",
    "encoded_example = tokenizer.encode_plus(example_text,\n",
    "                                        add_special_tokens=False)\n",
    "print(encoded_example.tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b421985",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "`bert-base-uncased` is trained with text in lower format.\n",
    "\n",
    "**Check model cards** on huggingface to know more about the models you use and inspect their text encoding pipeline to understand how they behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45979e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework \n",
    "\n",
    "Experiment with different model cards.\n",
    "\n",
    "Experiment with text cleaning and evaluate its impact on classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da8bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Model definition\n",
    "\n",
    "We are now ready to define our transformer-based classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a686b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Data Formatting\n",
    "\n",
    "We first need to format input data to be fed as mini-batches in a training/evaluation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db54f36f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc44786",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ``DataCollatorWithPadding`` receives a batch of\n",
    "\n",
    "```\n",
    "(input_ids, attention_mask, token_type_ids, label)\n",
    "```\n",
    "\n",
    "tuples and **dynamically pads** ``input_ids``, ``attention_mask`` and ``token_type_ids`` to maximum sequence in the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8705453",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, this operation saves a lot of memory compared to padding to global maximum sequence, while it introduces a reasonable computational overhead.\n",
    "\n",
    "### Note\n",
    "\n",
    "The above example is just one way out of many to perform dynamic batch padding: it really depends on which data structures you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46c5b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Model definition\n",
    "\n",
    "Defining a transformer-based model with huggingface is pretty straightforward!\n",
    "\n",
    "Since we are dealing with text classification, we can use off-the-shelf `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7888a387",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_card,\n",
    "    num_labels=2,\n",
    "    id2label={0: 'NEG', 1: 'POS'},\n",
    "    label2id={'NEG': 0, 'POS': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d8ded",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first check the loaded model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1378fff3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e6fa1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**That's it!**\n",
    "\n",
    "That's the simplicity of huggingface's APIs.\n",
    "\n",
    "The model is ready to use for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598b147",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3 Custom architectures\n",
    "\n",
    "There are plenty of pre-defined model architectures $\\rightarrow$ [auto classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "\n",
    "In more complex scenarios, we may want to define a custom architecture where the pre-trained model is part of it.\n",
    "\n",
    "In these cases, the way you do it strongly depends on the underlying neural library.\n",
    "\n",
    "However, there exist several high-level APIs depending on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd8e31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. Model training and evaluation\n",
    "\n",
    "We are now ready to define the training and evaluation procedures to test our model on the IMDB dataset.\n",
    "\n",
    "In particular, we are going to use ``Trainer`` APIs to efficiently perform training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fdb52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 Metrics\n",
    "\n",
    "First, we define classification metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc3550e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n",
    "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    return {'f1': f1, 'acc': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e2ad2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hugginface's metrics\n",
    "\n",
    "Huggingface's offers the **evaluate** package that contains several evaluation metrics (e.g., accuracy, f1, squad-f1, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36616ba0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc_metric = evaluate.load('accuracy')\n",
    "f1_metric = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "137d90dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(output_info):\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_metric.compute(predictions=predictions,\n",
    "                           references=labels,\n",
    "                           average='macro')\n",
    "    acc = acc_metric.compute(predictions=predictions, \n",
    "                             references=labels)\n",
    "    return {**f1, **acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb4224",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 Training Arguments\n",
    "\n",
    "The ``Trainer`` object can be extensively customized.\n",
    "\n",
    "Feel free to check the [documentation](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) on training arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b2df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first rename the `sentiment` column to `label` as the default input to `AutoModelForSequenceClassification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a002dcef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.rename_column('sentiment', 'label')\n",
    "test_data = test_data.rename_column('sentiment', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1c9b30a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_dir\",             # where to save model\n",
    "    learning_rate=2e-5,                   \n",
    "    per_device_train_batch_size=8,     # defines distributed training\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",             # when to report evaluation metrics/losses\n",
    "    save_strategy=\"epoch\",             # when to save checkpoint\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'                   # disabling wandb (default)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4071868",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f56baf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training schema with collator\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/collator.png\" alt=\"collator\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37827762",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 05:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.255287</td>\n",
       "      <td>0.904311</td>\n",
       "      <td>0.904320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=625, training_loss=0.3531867248535156, metrics={'train_runtime': 353.1258, 'train_samples_per_second': 14.159, 'train_steps_per_second': 1.77, 'total_flos': 622118649166176.0, 'train_loss': 0.3531867248535156, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bbd39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Evaluation\n",
    "\n",
    "We now evaluate the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0aacd3fa",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "test_prediction_info = trainer.predict(test_data)\n",
    "test_predictions = test_prediction_info.predictions\n",
    "test_labels = test_prediction_info.label_ids\n",
    "\n",
    "print(test_predictions.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07749c06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9043113276973538, 'accuracy': 0.90432}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = compute_metrics([test_predictions, test_labels])\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54212aed",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Some cleaning before PART II\n",
    "\n",
    "Let's clean the memory and GPU before switching to instruction-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af716d88",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "model = None\n",
    "del model\n",
    "trainer = None\n",
    "del trainer\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd0f67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PART II\n",
    "\n",
    "*   Prompting 101\n",
    "*   Sentiment analysis with prompting\n",
    "*   Advanced prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd914bb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Prompting 101\n",
    "\n",
    "Prompting is a technique used to adapt a model to a variety of tasks without requiring fine-tuning.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215c9d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The model receives the above input prompt and performs text classification via completion.\n",
    "\n",
    "```\n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: {text}\n",
    "Sentiment: {label}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac2bcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In natural language, prompting is a very delicate process since natural language is **expressive**, **flexible**, and, **ambiguous**.\n",
    "\n",
    "A certain concept can be expressed in several ways:\n",
    "\n",
    "* These ways are semantically **equivalent**\n",
    "* May lead to **significant** model performance **drifts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4af6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Sensitivity Factors\n",
    "\n",
    "There are two main factors to consider when performing prompt-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824d676",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Prompt Engineering](https://www.promptingguide.ai/)\n",
    "\n",
    "Eventually we have to iteratively find the best performing prompt.\n",
    "\n",
    "This can either done\n",
    "\n",
    "* Manually\n",
    "* Automatically (via an ad-hoc model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaed127",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### [Generation hyper-parameters](https://huggingface.co/docs/transformers/main/generation_strategies#text-generation-strategies)\n",
    "\n",
    "Finding the optimal text generation strategy is a **critical point** for achieving satisfying performance.\n",
    "\n",
    "These strategies affects how the model iteratively selects tokens during generation to avoid phenomena like repetitions, rare words, coherence with input text, and style.\n",
    "\n",
    "* [Deterministic] Greedy $\\rightarrow$ the most preferred (i.e., highest likelihood) token wins\n",
    "* [Deterministic] Beam search\n",
    "* [Stochastic] Top-k sampling\n",
    "* [Stochastic] Nucleus sampling (or top-p sampling)\n",
    "* [Contrastive search](https://huggingface.co/blog/introducing-csearch)  $\\leftarrow$ **recommended**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3603382",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.2 Model types\n",
    "\n",
    "There are a lot of different large language models and it is quite easy to be confused.\n",
    "\n",
    "Essentially, we have:\n",
    "\n",
    "* **Base models** (either encoders or encode-decoders): very good at text completion.\n",
    "* **Instruct-based models**: base models specifically fine-tuned to address instructions.\n",
    "* **Chat-based models**: models specifically fine-tuned to chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43bd15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "In Huggingface, the distinct is easily formatted as:\n",
    "\n",
    "* `llama2-7b`            $\\rightarrow$ base model\n",
    "* `llama2-7b-instruct`   $\\rightarrow$ instruct-based model\n",
    "* `llama-7b-chat`        $\\rightarrow$ chat-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65574c39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.3 Preliminaries\n",
    "\n",
    "We are going to download LLMs from [Huggingface](https://huggingface.co/).\n",
    "\n",
    "Many of these open-source LLMs require you to accept their \"Community License Agreement\" to download them.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- If not already, create an account of Huggingface (~2 mins)\n",
    "- Check a LLM model card page (e.g., [Mistral v3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)) and accept its \"Community License Agreement\".\n",
    "- Go to your account -> Settings -> Access Tokens -> Create new token -> \"Repositories permissions\" -> add the LLM model card you want to use.\n",
    "- Save the token (we'll need it later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4672f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once we have created an account and an access token, we need to login to Huggingface via code.\n",
    "\n",
    "- Type your token and press Enter\n",
    "- You can say No to Github linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc426bbc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfc4e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After login, you can download all models associated with your access token in addition to those that are not protected by an access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791323f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Sentiment analysis with prompting\n",
    "\n",
    "Let's consider our task once again to evaluate prompt-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115caa1c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Model pipeline\n",
    "\n",
    "First, we have to define the model pipeline to digest input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70442c79",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_card = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3f6ffef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbea75c42b846a5a88aa13f5b83a475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_card,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb30325e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 40\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.temperature = None\n",
    "generation_config.num_return_sequences = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ca902",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework \n",
    "\n",
    "Experiment with different model cards (either base or chat-base models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3af120",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Prompt Template\n",
    "\n",
    "We first define the prompt template to format data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4718ea96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sentiment analysis.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Classify the text into negative or positive.\n",
    "        Respond only POSITIVE or NEGATIVE.\n",
    "\n",
    "        TEXT: \n",
    "        {text}\n",
    "\n",
    "        SENTIMENT:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1af4a79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(prompt,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b0614",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's inspect the formatted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c28371a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an annotator for sentiment analysis.<|end|>\n",
      "<|user|>\n",
      "Classify the text into negative or positive.\n",
      "        Respond only POSITIVE or NEGATIVE.\n",
      "\n",
      "        TEXT: \n",
      "        {text}\n",
      "\n",
      "        SENTIMENT:\n",
      "        <|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865b844",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 Inference\n",
    "\n",
    "We are now ready to feed prompts to our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66afd8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e780436",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"This movie is definitely one of my favorite movies\n",
    "of its kind. The interaction between respectable and morally strong\n",
    "characters is an ode to chivalry and the honor code amongst thieves\n",
    "and policemen.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0cb7567",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "formatted_example = prompt.format(text=example_text)\n",
    "parsed_example = tokenizer(formatted_example,\n",
    "                           return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0121911",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an annotator for sentiment analysis. Classify the text into negative or positive.\n",
      "        Respond only POSITIVE or NEGATIVE.\n",
      "\n",
      "        TEXT: \n",
      "        This movie is definitely one of my favorite movies\n",
      "of its kind. The interaction between respectable and morally strong\n",
      "characters is an ode to chivalry and the honor code amongst thieves\n",
      "and policemen.\n",
      "\n",
      "        SENTIMENT:\n",
      "         POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786c2f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we try with the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61982dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_df = df.loc[df['split'] == \"test\"].sample(frac=1.0)[:100]\n",
    "test_data = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b083ec89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepend_prompt(example):\n",
    "    example['text'] = prompt.format(text=example['text'])\n",
    "    return example\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = tokenizer.batch_encode_plus([it['text'] for it in batch],\n",
    "                                        return_tensors='pt',\n",
    "                                        padding=True,\n",
    "                                        truncation=True)\n",
    "    sentiment = torch.tensor([it['sentiment'] for it in batch])\n",
    "    return texts, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0907d003",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517db4408406499583e0d53dc41707a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = test_data.map(prepend_prompt)\n",
    "test_data = test_data.select_columns(['text', 'sentiment'])\n",
    "data_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02e971",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before running the inference loop, we define a function to parse generated outputs into classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18baa5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_response(response):\n",
    "    match = [m for m in re.finditer('SENTIMENT:', response)][-1]\n",
    "    parsed = response[match.end():].strip()\n",
    "    return parsed\n",
    "\n",
    "def convert_response(response):\n",
    "    return [0, 1] if 'positive' in response.casefold() else [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9d73a78",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|| 100/100 [00:36<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_responses = []\n",
    "predictions = []\n",
    "with torch.inference_mode():\n",
    "    for batch_x, batch_y in tqdm(data_loader, \n",
    "                                 desc=\"Generating responses\"):\n",
    "        response = model.generate(\n",
    "            input_ids=batch_x['input_ids'].to(model.device),\n",
    "            attention_mask=batch_x['attention_mask'].to(model.device),\n",
    "            generation_config=generation_config,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        raw_response = tokenizer.batch_decode(response,\n",
    "                                              skip_special_tokens=True)\n",
    "        raw_response = [extract_response(item)\n",
    "                        for item in raw_response]\n",
    "        raw_responses.extend(raw_response)\n",
    "        batch_predictions = [convert_response(item)\n",
    "                             for item in raw_response]\n",
    "        predictions.extend(batch_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2fac0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now compute classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8c3a234",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9397832195905259, 'accuracy': 0.94}\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(test_data['sentiment'])\n",
    "metrics = compute_metrics([predictions, ground_truth])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219f128c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3. [Advanced Prompting](https://huggingface.co/docs/transformers/main/tasks/prompting#chain-of-thought)\n",
    "\n",
    "There is no rule of thumb to perform well on prompting.\n",
    "\n",
    "Some may argue it is *art*, some others might say it is just *engineering*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8b278",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, here are some **general recommendations**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e5b1a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Check **how** the pre-trained model you are using was trained!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71db06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Start **simple** and then refine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a46a65e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Instructions at the **start/end** of the prompt $\\rightarrow$ based on how most attention layers work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49fcee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Separate** input text from instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e007b29",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Provide **clear description** of the task: no ambiguity, text format, style, language, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab1762",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Evaluate** the prompt on several models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e669802",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use advanced techniques: **few-shot prompting**, **Chain-of-thought (CoT)**, Least-to-Most (LtM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27c309",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.1 From Zero- to Few-shot Prompting\n",
    "\n",
    "In many situations, a prompt containing instructions is not sufficient for a model to behave properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52c86c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can improve the prompt by providing **a few** ground-truth examples showing how the model should behave."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedb54a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "Classify the text into negative or positive. \n",
    "Text: {example1}\n",
    "Sentiment: {label1}\n",
    "Text: {example2}\n",
    "Sentiment: {label2}\n",
    "Text: {example3}\n",
    "Sentiment: {label3}\n",
    "Text: {text}\n",
    "Sentiment:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494c4ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples may be insufficient\n",
    "\n",
    "Depending on the task at hand, providing examples may be not sufficient for the model to *understand* the instructions.\n",
    "\n",
    "Also, the model might ignore provided examples or it might still perform correctly despite using **intentionally wrong** examples!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42471d5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lengthy prompts\n",
    "\n",
    "Adding examples increases the level of detail of prompt, while it may considerably increases its length.\n",
    "\n",
    "Pay attention to what ``model_card`` you choose since your model may **truncate** input prompts!\n",
    "\n",
    "Additionally, a lengthy prompt **increases computation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760aaae9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples quality\n",
    "\n",
    "Choosing the right set of examples has an impact on model performance.\n",
    "\n",
    "Intuitively, we select examples to maximize (textual) diversity and cover the whole label distribution.\n",
    "\n",
    "In practice, this may be harder than expected: models are sensitive to prompt formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec77bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try sentiment analysis again with Few-shot prompting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8130e78a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sentiment analysis.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Classify the text into negative or positive.\n",
    "        Respond only POSITIVE or NEGATIVE.\n",
    "        \n",
    "        Here are some examples you can look at.\n",
    "        EXAMPLES:\n",
    "        {examples}\n",
    "\n",
    "        Here is the text to classify.\n",
    "\n",
    "        TEXT: \n",
    "        {text}\n",
    "\n",
    "        SENTIMENT:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a84ffc80",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(prompt,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd3ef426",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "demonstrations = [\n",
    "    (\"\"\"Everything is so well done: acting, directing, visuals,\n",
    "    settings, photography, casting. If you can enjoy a story\n",
    "    of real people and real love - this is a winner.\"\"\", \"POSITIVE\"),\n",
    "    (\"\"\"This is one of the dumbest films, I've ever seen.\n",
    "    It rips off nearly ever type of thriller and manages\n",
    "    to make a mess of them all.\"\"\", \"NEGATIVE\")\n",
    "]\n",
    "demonstrations = '\\n'.join([f'TEXT: {text}\\nSENTIMENT: {sentiment}'\n",
    "                            for (text, sentiment) in demonstrations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed13cacf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"This movie is definitely one of my favorite movies\n",
    "of its kind. The interaction between respectable and morally strong\n",
    "characters is an ode to chivalry and the honor code amongst thieves\n",
    "and policemen.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a16bad65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "formatted_example = prompt.format(text=example_text,\n",
    "                                  examples=demonstrations)\n",
    "parsed_example = tokenizer(formatted_example,\n",
    "                           return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated,\n",
    "                                skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c902404",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an annotator for sentiment analysis. Classify the text into negative or positive.\n",
      "        Respond only POSITIVE or NEGATIVE.\n",
      "\n",
      "        Here are some examples you can look at.\n",
      "        EXAMPLES:\n",
      "        TEXT: Everything is so well done: acting, directing, visuals,\n",
      "    settings, photography, casting. If you can enjoy a story\n",
      "    of real people and real love - this is a winner.\n",
      "SENTIMENT: POSITIVE\n",
      "TEXT: This is one of the dumbest films, I've ever seen.\n",
      "    It rips off nearly ever type of thriller and manages\n",
      "    to make a mess of them all.\n",
      "SENTIMENT: NEGATIVE\n",
      "\n",
      "        Here is the text to classify.\n",
      "\n",
      "        TEXT: \n",
      "        This movie is definitely one of my favorite movies\n",
      "of its kind. The interaction between respectable and morally strong\n",
      "characters is an ode to chivalry and the honor code amongst thieves\n",
      "and policemen.\n",
      "\n",
      "        SENTIMENT:\n",
      "         POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "507c5d77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def prepend_prompt(example):\n",
    "    example['text'] = prompt.format(text=example['text'],\n",
    "                                    examples=demonstrations)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "114dcf36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a99ae53bf14b4588e569d2b9c897cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = Dataset.from_pandas(test_df)\n",
    "test_data = test_data.map(prepend_prompt)\n",
    "test_data = test_data.select_columns(['text', 'sentiment'])\n",
    "data_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1ca5008",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|| 100/100 [00:42<00:00,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_responses = []\n",
    "predictions = []\n",
    "with torch.inference_mode():\n",
    "    for batch_x, batch_y in tqdm(data_loader,\n",
    "                                 desc=\"Generating responses\"):\n",
    "        response = model.generate(\n",
    "            input_ids=batch_x['input_ids'].to(model.device),\n",
    "            attention_mask=batch_x['attention_mask'].to(model.device),\n",
    "            generation_config=generation_config,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        raw_response = tokenizer.batch_decode(response,\n",
    "                                              skip_special_tokens=True)\n",
    "        raw_response = [extract_response(item)\n",
    "                        for item in raw_response]\n",
    "        raw_responses.extend(raw_response)\n",
    "        batch_predictions = [convert_response(item)\n",
    "                             for item in raw_response]\n",
    "        predictions.extend(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e3c1c3c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9198717948717949, 'accuracy': 0.92}\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(test_data['sentiment'])\n",
    "metrics = compute_metrics([predictions, ground_truth])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f255b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework \n",
    "\n",
    "Experiment with different few-shot examples and evaluate corresponding model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3412321",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.2 Chain-of-thought (CoT) Prompting\n",
    "\n",
    "Providing examples to improve task performance may fail in complex scenarios like reasoning tasks.\n",
    "\n",
    "CoT prompting forces the model to generate intermediate reasoning steps before providing the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05358f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CoT can either be achieved via\n",
    "\n",
    "* Few-shot examples on how to perform *reasoning*\n",
    "* Defining the prompt to force *reasoning* (e.g., *let's think step by step*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298b790",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try our sentiment analysis task with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56678577",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an annotator for sentiment analysis.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"\"\"Classify the text into negative or positive.\n",
    "        Respond with POSITIVE or NEGATIVE.\n",
    "        Think step by step.\n",
    "        \n",
    "        Here are some examples you can look at.\n",
    "        EXAMPLES:\n",
    "        {examples}\n",
    "\n",
    "        Here is the text to classify.\n",
    "\n",
    "        TEXT: \n",
    "        {text}\n",
    "\n",
    "        SENTIMENT:\n",
    "        \"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c26e595",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(prompt, \n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83e5daef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"This movie is definitely one of my favorite movies\n",
    "of its kind. The interaction between respectable and morally strong\n",
    "characters is an ode to chivalry and the honor code amongst thieves\n",
    "and policemen.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46702439",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "formatted_example = prompt.format(text=example_text,\n",
    "                                  examples=demonstrations)\n",
    "parsed_example = tokenizer(formatted_example, \n",
    "                           return_tensors='pt').to('cuda')\n",
    "generated = model.generate(input_ids=parsed_example['input_ids'],\n",
    "                           attention_mask=parsed_example['attention_mask'],\n",
    "                           generation_config=generation_config,\n",
    "                           do_sample=False)\n",
    "output = tokenizer.batch_decode(generated, \n",
    "                                skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c93c753",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an annotator for sentiment analysis. Classify the text into negative or positive.\n",
      "        Respond with POSITIVE or NEGATIVE.\n",
      "        Think step by step.\n",
      "\n",
      "        Here are some examples you can look at.\n",
      "        EXAMPLES:\n",
      "        TEXT: Everything is so well done: acting, directing, visuals,\n",
      "    settings, photography, casting. If you can enjoy a story\n",
      "    of real people and real love - this is a winner.\n",
      "SENTIMENT: POSITIVE\n",
      "TEXT: This is one of the dumbest films, I've ever seen.\n",
      "    It rips off nearly ever type of thriller and manages\n",
      "    to make a mess of them all.\n",
      "SENTIMENT: NEGATIVE\n",
      "\n",
      "        Here is the text to classify.\n",
      "\n",
      "        TEXT: \n",
      "        This movie is definitely one of my favorite movies\n",
      "of its kind. The interaction between respectable and morally strong\n",
      "characters is an ode to chivalry and the honor code amongst thieves\n",
      "and policemen.\n",
      "\n",
      "        SENTIMENT:\n",
      "         POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7ae1858",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e72276c9ed4557bed6998d5bed49e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = Dataset.from_pandas(test_df)\n",
    "test_data = test_data.map(prepend_prompt)\n",
    "test_data = test_data.select_columns(['text', 'sentiment'])\n",
    "data_loader = DataLoader(test_data,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False,\n",
    "                         collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a96b5690",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|| 100/100 [00:43<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_responses = []\n",
    "predictions = []\n",
    "with torch.inference_mode():\n",
    "    for batch_x, batch_y in tqdm(data_loader,\n",
    "                                 desc=\"Generating responses\"):\n",
    "        response = model.generate(\n",
    "            input_ids=batch_x['input_ids'].to(model.device),\n",
    "            attention_mask=batch_x['attention_mask'].to(model.device),\n",
    "            generation_config=generation_config,\n",
    "            do_sample=False,\n",
    "            use_cache=True\n",
    "        )\n",
    "        raw_response = tokenizer.batch_decode(response, \n",
    "                                              skip_special_tokens=True)\n",
    "        raw_response = [extract_response(item)\n",
    "                        for item in raw_response]\n",
    "        raw_responses.extend(raw_response)\n",
    "        batch_predictions = [convert_response(item)\n",
    "                             for item in raw_response]\n",
    "        predictions.extend(batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c895837",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.9597423510466989, 'accuracy': 0.96}\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions)\n",
    "ground_truth = np.array(test_data['sentiment'])\n",
    "metrics = compute_metrics([predictions, ground_truth])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34584cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Homework \n",
    "\n",
    "Experiment with different CoT prompts to enforce intermediate reasoning steps.\n",
    "\n",
    "For more details check this [page](https://www.promptingguide.ai/techniques/cot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5990dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Prompting vs Fine-tuning\n",
    "\n",
    "At last, we may wondering on which technique to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8c810",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In short, prompting comes at hand when transferring a pre-trained model on a domain that has some affinities with those seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f292f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other cases like:\n",
    "\n",
    "* Different domain\n",
    "* Sensitive data\n",
    "* Low-resource language\n",
    "* Domain-specific model constraints\n",
    "\n",
    "Fine-tuning is the preferred choice (to maximize improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e09c5",
   "metadata": {
    "id": "UbzMCMfprp3m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
